{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = wx1 + (1-w)x2\n",
    "# 직선(line): 시작과 끝 지점이 존재하지 않음\n",
    "# 선분(line segment): 시작과 끝 지점이 존재함\n",
    "\n",
    "# affine set\n",
    "# (x1, x2 ∈ C), (wx1 + (1-w)x2 ∈ C) 일떄 집합 C\n",
    "# w 값이 제한이 없어 직선에 해당\n",
    "# affine combination: n차원으로 확장한 w1x1 + w2x2 + ... + wnxn\n",
    "\n",
    "# linear function: f(x) = Wx\n",
    "# affine function: f(x) = Wx + b\n",
    "\n",
    "# convex set\n",
    "# (x1, x2 ∈ C), (wx1 + (1-w)x2 ∈ C), (0 ≤ w ≤ 1) 일떄 집합 C\n",
    "# 집합 C 내부의 두 점 사이의 선분이 집합 C에 속하는 것\n",
    "\n",
    "# affine set: 두 점을 잇는 직선 포함, 가중치 w 값의 범위 제한 없음\n",
    "# convex set: 두 점을 잇는 선분 포함, 가중치 w 값의 범위 제한 있음\n",
    "\n",
    "# convex hull: 주어진 점들을 포함하는 convex set\n",
    "\n",
    "# hyperplane: {x|w.Tx=b}\n",
    "# w.Tx=b 조건을 만족하는 데이터 포인트 x의 집합\n",
    "\n",
    "# halfspace: 초평면에 의해 두 개의 반공간으로 나뉨\n",
    "# w.Tx ≥ b, w.Tx ≤ b\n",
    "# 머신러닝 목적은 효과적으로 나눌 수 있는 초평면 찾는 문제\n",
    "\n",
    "# convex function\n",
    "# f(wx1 + (1-w)x2) ≤ wf(x1) + (1-w)f(x2) 만족하면 convex하다\n",
    "# (x1, f(x1)), (x2, f(x2)) 선분이 함수 f의 그래프보다 위에 있다\n",
    "\n",
    "# ∇f: gradient, 모든 점에 대해 해당 점의 미분 값을 의미\n",
    "# 0이면 최소값 성립\n",
    "\n",
    "# convex optimization\n",
    "# f(x): 목적 함수, convex function\n",
    "# x: 최적화 변수\n",
    "# gi(x) ≤ 0: inequality constraint function, convex function\n",
    "# hi(x) = wi.Tx-bi = 0: equality constraint function, affine function\n",
    "\n",
    "# Lagrange primal function\n",
    "# Lp(x, λ, v) = f(x) + ∑λigi(x) + ∑vihi(x)\n",
    "# λi, vi: dual vector, Lagrange multiplier vector\n",
    "\n",
    "# Lagrange dual function\n",
    "# 라그랑주 프리멀 함수의 하한\n",
    "# Ld(λ, v) ≤ p* (λ ≥ 0)\n",
    "# weak duality: d* ≤ p*\n",
    "# duality gap: p* - d* (nonnegative)\n",
    "\n",
    "# KKT(Karush-Kuhn-Tucker)\n",
    "# fi, gi convex function, hi affine function 일때, KKT 조건을 만족하면,\n",
    "#  프리멀 문제와 듀얼 문제에서의 최적값이며 듀얼리티 갭이 0\n",
    "# Primal feasibility: gi(x*) ≤ 0, hi(x*) = 0\n",
    "# Dual feasibility: λi* ≥ 0\n",
    "# Complementary slackness: λi*gi(x*) = 0\n",
    "# Stationarity: ∇f(x*) + ∑λi*gi(x*) + ∑vi*hi(x*) = 0\n",
    "# 컨벡스 최적화 문제에서 목적 함수와 제약 함수가 미분 가능할 때,\n",
    "#  KTT 조건을 만족하는 지점은 zero duality gap → 프리멀 듀얼 최적값을 의미\n",
    "\n",
    "# 최소 제곱 법\n",
    "# min((y-Xw).T(y-Xw)) → w = (X.T*X)-1*X.T*y\n",
    "\n",
    "# Optimizer: 최적화 목적 함수인 손실 함수 기반으로 어떻게 업데이트할지 결정\n",
    "# Gradient descent: 경사 하강법, 비용 함수의 미분 값 이용 (미분 값의 반대 방향으로 이동)\n",
    "#  θt+1 = θt + delta(θt), delta(θt) = -η∇J(θt) (η: learning rate)\n",
    "#  η가 너무 크면 이동 값이 커져 최적값을 구하지 못하며, 너무 작으면 많은 시간 소요\n",
    "#  (Batch/Stochastic/Mini-Batch) Gradient descent\n",
    "#  Stochastic 장점: 속도 빠름, 리소스 적음 / 단점: 최적해 벗어날 수 있음, 학습률 정하기 어려움 (최적해 벗어나면 학습률 낮추기)\n",
    "# Momentum: 학습 속도를 가속화\n",
    "# θ(t+1) = θ(t) - v(t), v(t) = γ*v(t-1) + η∇J(θt), γ = 0.9\n",
    "\n",
    "# Confusion Matrix\n",
    "# accuracy, error rate, sensitivity(recall), precision, false postive rate, f1 score\n",
    "\n",
    "# Cross validation\n",
    "# train/validation/test, train model fit → validation hyper parameter tune → 반복 후 test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN(K Nearest Neighbor): 거리 기반, 가까이 있는 데이터는 같은 Label\n",
    "#  lazy learning (eager learning)\n",
    "#  분류(근처 다수결)/회귀(근처 평균)\n",
    "\n",
    "# 선형 회귀: 오차 제곱합 최소화\n",
    "# 회귀 분석 가정:\n",
    "#  1) E(y) = B1x + B0, E(y) = wx + b\n",
    "#  2) feature x가 주어졌을 때 target y는 정규분포를 따른다\n",
    "#  3) 이때 y 평균은 E(y) = wx + b로 x 값에 따라 변하나 분산은 x 값에 관계 없이 일정하다\n",
    "#  4) x는 오차 없이 측정할 수 있는 수학 변수(mathematical variable), y는 측정 오차를 수반하는 확률 변수(random variable)\n",
    "#  5) y의 오차들은 서로 독립\n",
    "# 최소 제곱 추정량(least square estimator): 오차 제곱합을 최소로 하는 w, b 구하기\n",
    "# SST = SSE + SSR, r2 = SSR/SST (0 ≤ r2 ≤ 1)\n",
    "# 다중공선성(multicollinearity): 선형 조합으로 표현 가능, x1 = x2 + x3\n",
    "# 차원 축소: 카이제곱 검정 이용\n",
    "# Ridge 회귀 분석: argmin((y-X*w).T*(y-X*w) + λ(|w|2-t))\n",
    "#  λ는 계수 사이즈/ 정규식 크기 조절, 0이면 최소 제곱 추정량, 무한대면 0에 가까워지고 상수항만 남은 모형\n",
    "# Lasso 회귀 분석: argmin((y-X*w).T*(y-X*w) + λ(|w|-t))\n",
    "#  closed form solution 존재하지 않아 경사하강법 같은 numerical optimization 사용해서 구함\n",
    "# 제약식 일반화된 표현: argmin((y-X*w).T*(y-X*w) + λ|w|q-t), q = [4, 2, 1, 0.5, 0.1]\n",
    "# Elastic net 회귀 분석: argmin((y-X*w).T*(y-X*w) + λ(a|w|+(1-a)|w|2-t)))\n",
    "\n",
    "# Logistic 회귀 분석(분류)\n",
    "# y = 1/(1+exp(-z)), log(y/(1-y))\n",
    "# log(y/(1-y)): logit, y/(1-y): odds ratio\n",
    "# x = -b/w일때 급격하게 0 or 1로 수렴\n",
    "\n",
    "# 나이브 베이즈(분류)\n",
    "# feature들이 조건부 독립\n",
    "\n",
    "# 의사결정나무\n",
    "# 노드 별 엔트로피(무질서 정도)를 측정 후, 테스트 전체 퀄리티 측정\n",
    "# Entropy = -(P/T)log(P/T) - (N/T)log(N/T)\n",
    "# 지니계수 = (P/T)(1-P/T) + (N/T)(1-N/T)\n",
    "# feature 연속형: 전부 테스트 후보, target 연속형: 평균값으로 추정\n",
    "\n",
    "# Support Vector Machine\n",
    "# 서포트 벡터간 너비 최대화 → min(square(|w|)/2) → w 자신과 내적 최소화\n",
    "# 목적 함수: min(square(|w|)/2), 제약 조건: yi(w*x+b)=0 (yi=1(x+), -1(x-))\n",
    "# Decision Rule: wx + b > 0, w = ∑aiyixi\n",
    "# 서포트 벡터 머신 회귀: Ie(x) = 0(|x|<e), |x|-e(otherwise)\n",
    "\n",
    "# Ensemble learning\n",
    "# 여러가지 지도학습 알고리즘 조합해서 학습, 분루 모형을 여러개 만들어 비교\n",
    "# 개별 분류 모형을 분류기(classifier), 성능이 낮은 분류기를 조합해 성능 좋은 분류기 생성\n",
    "# Voting: 여러가지 지도학습 알고리즘의 결과를 투표를 통해 최종 Label 지정\n",
    "# Bagging: 개별 분류기들의 분류 결과 종합 후, 최종 분류기 성능 향상 (bootstrap: 중복 허용 랜덤 샘플)\n",
    "# Random Forest: bootstrap(n개 랜덤 데이터 샘플, 중복 가능), d개 feature(중복 불가능), 학습, 투표 통해 class 할당\n",
    "# Boosting: 분류 어려운 데이터에 가중치 할당(잘 된 건 가중치 감소), Ada boost/Gradient boosting (비용 함수를 최소화 하는 방향)\n",
    "# Voting, Bagging(병렬) vs Boosting(직렬, 순서대로 진행해서 시간 소요)\n",
    "# Stacking: base learner(먼저 학습) + meta learner(base 예측을 feature data로 활용해 최종 예측)\n",
    "\n",
    "# Unsupervised learning\n",
    "# k means clustering: 거리 기반 (silhouette_score, centeroid가 멀면 커짐, 0.3보다 크면 잘나온 것)\n",
    "# DBSCAN: 밀도 기반\n",
    "# GMM: 전체 집단의 확률 분포가 가우시안 분포를 따르는 경우\n",
    "\n",
    "# 이상치 탐지\n",
    "# One class SVM: nu는 이상치 비율이며 0 ≤ nu ≤ 1\n",
    "# Local Outlier Factor: 이웃 밀도 고려, 1보다 크면 이상치\n",
    "# Isolation Forest: 이상치 쉽게 고립시키기 가능\n",
    "\n",
    "# 차원 축소\n",
    "# 차원의 저주: n차원 데이터 채우기 힘듬 (row의 feature개수 승 만큼 데이터 필요, x10, row_num = 10, 10^10)\n",
    "# Principal component analysis: 주성분벡터로 정사영, 새로운 좌표축 기준 데이터 표현\n",
    "# Linear discriminant analysis: 선형 판별 분석, 판별 함수가 선형 형태의 함수\n",
    "#  집단 간 분산과 집단 내 분산의 비율을 최대화 하는 새로운 공간으로 차원 축소 (집단 내 분산의 역행렬과 집단 간 분산의 내적 값, 고유값 분해)\n",
    "\n",
    "# 시계열 분석 (시험 범위 제외)\n",
    "# Stationarity(정상성): 시계열의 확률적인 성질들이 시간의 흐름에 따라 불변(평균, 분산 등이 시간의 흐름에 따라 체계적/주기적 변화가 없는 것)\n",
    "# AR(p): 과거 나 자신의 값으로 예측 - Auto Regressive process\n",
    "# MA(q): 과거 오차로 예측 - Moving Average process\n",
    "# ARMA(p, q): 약한 정상성(평균, 분산 일정) 데이터, ARMAX\n",
    "# ARIMA(p, d, q): 평균 변동, 분산 일정 데이터, ARIMAX\n",
    "# SARIMA(p, d, q)(P, D, Q)s: ARIMA + 계절성, SARIMAX - S: Seasonal\n",
    "# SARIMA(p, d, q)(P, D, Q)s + GARCH(s, r): SARIMA + 분산 변동 데이터, SARIMAX + GARCH - Generalized ARCH\n",
    "# ARCH: 과거의 오차\n",
    "# GARCH: 과거의 분산값까지 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class: 객체를 만들어내기 위한 설계도 (변수와 메서드 집합)\n",
    "# instance: 설계도를 바탕으로 구현된 구체적인 실체 (객체가 메모리에 할당되어 실제 사용될 때)\n",
    "# method overriding: 부모 클래스의 메소드를 자식 클래스에서 재정의\n",
    "# 인공신경망(Artifical Neural Network)\n",
    "#  인간의 뇌는 뉴런이란 세포로 구성, 뉴런(노드)은 시냅스로 연결, 자극의 크기가 특정 값 이상이면 뉴런에 전달\n",
    "# 딥러닝: 신경망 기반 학습, 수많은 퍼셉트론이 연결되어 결과 주고 받음, 비선형 구조\n",
    "# 입력층(input layer), 은닉층(hidden layer), 출력층(output layer)\n",
    "# 퍼셉트론 구조\n",
    "#  입력값(1,x1,x2) → 가중합(wx1+wx2+b) → 활성화 함수(sigmoid(wx1+wx2+b)) → 출력값(y)\n",
    "#  가중합(weighted sum), 활성화 함수(activating function)\n",
    "#  활성화 함수 Φ(x)\n",
    "#   step: 0(t≤0), 1(t>0)\n",
    "#   sign: 1(x>0), 0(x=0), -1(x<0)\n",
    "#   sigmoid: 1/(1-e^(-x)) → 로지스틱 회귀에서도 사용\n",
    "#   hyperbolic tangent: (e^x-e^(-x))/(e^x+e^(-x)) → -1 ~ 1\n",
    "#   relu: x(x>0, 가중합), 0(x≤0)\n",
    "#   leaky relu: x(x>0), ax(x≤0)\n",
    "#   identity: x, 회귀 문제\n",
    "#   softmax: e^x/∑e^x (분류 문제, 최종층에서 사용)\n",
    "#   cross entropy: 0≤-∑(P(x))log(Q(x))≤무한대 (0에 가까울수록 분류 잘됨)\n",
    "# 신경망(Neural Network): 다수 퍼셉트론 활용(다층 퍼셉트론)\n",
    "# 딥러닝할 때 고민할 것: Hidden layer의 층수/ 각층의 노드 개수/ 각층의 활성화 함수\n",
    "#  Hidden layer 외곽이 ^ ~ 이런 형태 추천, \\_ 줄어드는 경우는 비추천\n",
    "#  Input layer 개수의 1.5~2배로 시작\n",
    "# 순전파(forward propagation)와 역전파(backpropagation) 반복해서 w, b 구함\n",
    "# 오차 역전파(backpropagation)\n",
    "#  1. 가중치 초기화\n",
    "#  2. 순전파를 통한 출력 값 계산\n",
    "#  3. 비용 함수 정의 및 1차 미분식 구하기\n",
    "#  4. 역전파를 통한 1차 미분 값 구하기\n",
    "#  5. 파라미터 업데이트\n",
    "#  6. 2~6 반복\n",
    "# Standardization → 배치 정규화(batch normalization, gradient 소실 문제 줄임으로 속도 향상)\n",
    "# Drop out: 일부 노드를 사용하지 않아 overfitting 방지\n",
    "# 전체 1,000개, 트레인 700개(미니배치 100개씩 7개), 테스트 300개 → iteration 7번 돌면 1 epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_11_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
