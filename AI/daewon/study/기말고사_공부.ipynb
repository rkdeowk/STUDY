{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = wx1 + (1-w)x2\n",
    "# 직선(line): 시작과 끝 지점이 존재하지 않음\n",
    "# 선분(line segment): 시작과 끝 지점이 존재함\n",
    "\n",
    "# affine set\n",
    "# (x1, x2 ∈ C), (wx1 + (1-w)x2 ∈ C) 일떄 집합 C\n",
    "# w 값이 제한이 없어 직선에 해당\n",
    "# affine combination: n차원으로 확장한 w1x1 + w2x2 + ... + wnxn\n",
    "\n",
    "# linear function: f(x) = Wx\n",
    "# affine function: f(x) = Wx + b\n",
    "\n",
    "# convex set\n",
    "# (x1, x2 ∈ C), (wx1 + (1-w)x2 ∈ C), (0 ≤ w ≤ 1) 일떄 집합 C\n",
    "# 집합 C 내부의 두 점 사이의 선분이 집합 C에 속하는 것\n",
    "\n",
    "# affine set: 두 점을 잇는 직선 포함, 가중치 w 값의 범위 제한 없음\n",
    "# convex set: 두 점을 잇는 선분 포함, 가중치 w 값의 범위 제한 있음\n",
    "\n",
    "# convex hull: 주어진 점들을 포함하는 convex set\n",
    "\n",
    "# hyperplane: {x|w.Tx=b}\n",
    "# w.Tx=b 조건을 만족하는 데이터 포인트 x의 집합\n",
    "\n",
    "# halfspace: 초평면에 의해 두 개의 반공간으로 나뉨\n",
    "# w.Tx ≥ b, w.Tx ≤ b\n",
    "# 머신러닝 목적은 효과적으로 나눌 수 있는 초평면 찾는 문제\n",
    "\n",
    "# convex function\n",
    "# f(wx1 + (1-w)x2) ≤ wf(x1) + (1-w)f(x2) 만족하면 convex하다\n",
    "# (x1, f(x1)), (x2, f(x2)) 선분이 함수 f의 그래프보다 위에 있다\n",
    "\n",
    "# ∇f: gradient, 모든 점에 대해 해당 점의 미분 값을 의미\n",
    "# 0이면 최소값 성립\n",
    "\n",
    "# convex optimization\n",
    "# f(x): 목적 함수, convex function\n",
    "# x: 최적화 변수\n",
    "# gi(x) ≤ 0: inequality constraint function, convex function\n",
    "# hi(x) = wi.Tx-bi = 0: equality constraint function, affine function\n",
    "\n",
    "# Lagrange primal function\n",
    "# Lp(x, λ, v) = f(x) + ∑λigi(x) + ∑vihi(x)\n",
    "# λi, vi: dual vector, Lagrange multiplier vector\n",
    "\n",
    "# Lagrange dual function\n",
    "# 라그랑주 프리멀 함수의 하한\n",
    "# Ld(λ, v) ≤ p* (λ ≥ 0)\n",
    "# weak duality: d* ≤ p*\n",
    "# duality gap: p* - d* (nonnegative)\n",
    "\n",
    "# KKT(Karush-Kuhn-Tucker)\n",
    "# fi, gi convex function, hi affine function 일때, KKT 조건을 만족하면,\n",
    "#  프리멀 문제와 듀얼 문제에서의 최적값이며 듀얼리티 갭이 0\n",
    "# Primal feasibility: gi(x*) ≤ 0, hi(x*) = 0\n",
    "# Dual feasibility: λi* ≥ 0\n",
    "# Complementary slackness: λi*gi(x*) = 0\n",
    "# Stationarity: ∇f(x*) + ∑λi*gi(x*) + ∑vi*hi(x*) = 0\n",
    "# 컨벡스 최적화 문제에서 목적 함수와 제약 함수가 미분 가능할 때,\n",
    "#  KTT 조건을 만족하는 지점은 zero duality gap → 프리멀 듀얼 최적값을 의미\n",
    "\n",
    "# 최소 제곱 법\n",
    "# min((y-Xw).T(y-Xw)) → w = (X.T*X)-1*X.T*y\n",
    "\n",
    "# Optimizer: 최적화 목적 함수인 손실 함수 기반으로 어떻게 업데이트할지 결정\n",
    "# Gradient descent: 경사 하강법, 비용 함수의 미분 값 이용 (미분 값의 반대 방향으로 이동)\n",
    "#  θt+1 = θt + delta(θt), delta(θt) = -η∇J(θt) (η: learning rate)\n",
    "#  η가 너무 크면 이동 값이 커져 최적값을 구하지 못하며, 너무 작으면 많은 시간 소요\n",
    "#  (Batch/Stochastic/Mini-Batch) Gradient descent\n",
    "#  Stochastic 장점: 속도 빠름, 리소스 적음 / 단점: 최적해 벗어날 수 있음, 학습률 정하기 어려움 (최적해 벗어나면 학습률 낮추기)\n",
    "# Momentum: 학습 속도를 가속화\n",
    "# θ(t+1) = θ(t) - v(t), v(t) = γ*v(t-1) + η∇J(θt), γ = 0.9\n",
    "\n",
    "# Confusion Matrix\n",
    "# accuracy, error rate, sensitivity(recall), precision, false postive rate, f1 score\n",
    "\n",
    "# Cross validation\n",
    "# train/validation/test, train model fit → validation hyper parameter tune → 반복 후 test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN(K Nearest Neighbor): 거리 기반, 가까이 있는 데이터는 같은 Label\n",
    "#  lazy learning (eager learning)\n",
    "#  분류(근처 다수결)/회귀(근처 평균)\n",
    "\n",
    "# 선형 회귀: 오차 제곱합 최소화\n",
    "# 최소 제곱 추정량(least square estimator): 오차 제곱합을 최소로 하는 w, b 구하기\n",
    "# SST = SSE + SSR, r2 = SSR/SST (0 ≤ r2 ≤ 1)\n",
    "# 다중공선성(multicollinearity): 선형 조합으로 표현 가능, x1 = x2 + x3\n",
    "# 차원 축소: 카이제곱 검정 이용\n",
    "# Ridge 회귀 분석: argmin((y-X*w).T*(y-X*w) + λ(|w|2-t))\n",
    "#  λ는 계수 사이즈/ 정규식 크기 조절, 0이면 최소 제곱 추정량, 무한대면 0에 가까워지고 상수항만 남은 모형\n",
    "# Lasso 회귀 분석: argmin((y-X*w).T*(y-X*w) + λ(|w|-t))\n",
    "#  closed form solution 존재하지 않아 경사하강법 같은 numerical optimization 사용해서 구함\n",
    "# 제약식 일반화된 표현: argmin((y-X*w).T*(y-X*w) + λ|w|q-t), q = [4, 2, 1, 0.5, 0.1]\n",
    "# Elastic net 회귀 분석: argmin((y-X*w).T*(y-X*w) + λ(a|w|+(1-a)|w|2-t)))\n",
    "\n",
    "# Logistic 회귀 분석(분류)\n",
    "# y = 1/(1+exp(-z)), log(y/(1-y))\n",
    "# log(y/(1-y)): logit, y/(1-y): odds ratio\n",
    "# x = -b/w일때 급격하게 0 or 1로 수렴\n",
    "\n",
    "# 나이브 베이즈(분류)\n",
    "# feature들이 조건부 독립\n",
    "\n",
    "# 의사결정나무\n",
    "# 노드 별 엔트로피(무질서 정도)를 측정 후, 테스트 전체 퀄리티 측정\n",
    "# Entropy = -(P/T)log(P/T) - (N/T)log(N/T)\n",
    "# 지니계수 = (P/T)(1-P/T) + (N/T)(1-N/T)\n",
    "# feature 연속형: 전부 테스트 후보, target 연속형: 평균값으로 추정\n",
    "\n",
    "# Support Vector Machine\n",
    "# 서포트 벡터간 너비 최대화 → min(square(|w|)/2) → w 자신과 내적 최소화\n",
    "# 목적 함수: min(square(|w|)/2), 제약 조건: yi(w*x+b)=0 (yi=1(x+), -1(x-))\n",
    "# 서포트 벡터 머신 회귀: Ie(x) = 0(|x|<e), |x|-e(otherwise)\n",
    "\n",
    "# Ensemble learning\n",
    "# 여러가지 지도학습 알고리즘 조합해서 학습, 분루 모형을 여러개 만들어 비교\n",
    "# 개별 분류 모형을 분류기(classifier), 성능이 낮은 분류기를 조합해 성능 좋은 분류기 생성\n",
    "# Voting: 여러가지 지도학습 알고리즘의 결과를 투표를 통해 최종 Label 지정\n",
    "# Bagging: 개별 분류기들의 분류 결과 종합 후, 최종 분류기 성능 향상 (bootstrap: 중복 허용 랜덤 샘플)\n",
    "# Random Forest: bootstrap(n개 랜덤 데이터 샘플, 중복 가능), d개 feature(중복 불가능), 학습, 투표 통해 class 할당\n",
    "# Boosting: 분류 어려운 데이터에 가중치 할당(잘 된 건 가중치 감소), Ada boost/Gradient boosting\n",
    "# Voting, Bagging(병렬) vs Boosting(직렬, 순서대로 진행해서 시간 소요)\n",
    "# Stacking: base learner(먼저 학습) + meta learner(base 예측을 feature data로 활용해 최종 예측)\n",
    "\n",
    "# Unsupervised learning\n",
    "# k means clustering: 거리 기반 (silhouette_score, centeroid가 멀면 커짐, 0.3보다 크면 잘나온 것)\n",
    "# DBSCAN: 밀도 기반\n",
    "# GMM: 전체 집단의 확률 분포가 가우시안 분포를 따르는 경우\n",
    "\n",
    "# 이상치 탐지\n",
    "# One class SVM: nu는 이상치 비율이며 0 ≤ nu ≤ 1\n",
    "# Local Outlier Factor: 이웃 밀도 고려, 1보다 크면 이상치\n",
    "# Isolation Forest: 이상치 쉽게 고립시키기 가능\n",
    "\n",
    "# 차원 축소\n",
    "# 차원의 저주: n차원 데이터 채우기 힘듬 (row의 feature개수 승 만큼 데이터 필요, x10, row_num = 10, 10^10)\n",
    "# Principal component analysis: 주성분벡터로 정사영, 새로운 좌표축 기준 데이터 표현\n",
    "# Linear discriminant analysis: 선형 판별 분석, 판별 함수가 선형 형태의 함수\n",
    "#  집단 간 분산과 집단 내 분산의 비율을 최대화 하는 새로운 공간으로 차원 축소 (집단 내 분산의 역행렬과 집단 간 분산의 내적 값, 고유값 분해)\n",
    "\n",
    "# 시계열 분석\n",
    "# AR(p): 과거 나 자신의 값으로 예측 - Auto Regressive process\n",
    "# MA(q): 과거 오차로 예측 - Moving Average process\n",
    "# ARMA(p, q): 약한 정상성(평균, 분산 일정) 데이터, ARMAX\n",
    "# ARIMA(p, d, q): 평균 변동, 분산 일정 데이터, ARIMAX\n",
    "# SARIMA(p, d, q)(P, D, Q)s: ARIMA + 계절성, SARIMAX - S: Seasonal\n",
    "# SARIMA(p, d, q)(P, D, Q)s + GARCH(s, r): SARIMA + 분산 변동 데이터, SARIMAX + GARCH - Generalized ARCH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_11_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
