{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **기획**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### <기획 가이드>\n",
    "> #### 현업 적용 문제 정의\n",
    "(공유 가능한 선에서 업무를 소개해 주세요)\n",
    "- **요구사항 확인 :** 현업에서의 분석 요구사항, 요구사항이 나오게 된 배경 등 과 같은 실제 현업의 문제점 작성.\n",
    "- **문제 범위 설정 :** 인텐시브 과정 내 가능한 수준인가, 분석 범위 및 계획 등과 같은 범위를 설정.\n",
    "- **문제 정의 도출 :** 위 내용을 바탕으로 해결하고 싶은 문제(가설) 정의.<br> (main 정의 후 main을 해결하기 위한 sub 문제 2-3개)\n",
    "\n",
    "> #### 데이터 수집 방안 및 사용 데이터 정의\n",
    "- **데이터 확보 방법 :** 사내 DB 추출, 웹데이터 크롤링 등 확보 방법을 작성해 주세요.\n",
    "- **활용 데이터 정의 :** 데이터 소스(사내 시스템명, 외부 사이트 URL 등), 데이터 유형(정형, 비정형_텍스트, 이미지, 영상 등), Feature, 형태(확장자, shape, size, count) 등 데이터에 관한 설명<br>\n",
    "\n",
    "> #### 분석 방향 설정\n",
    "- **분석 시 예상되는 어려움 :** 논리적 추론을 통해 문제를 해결해 나가는 과정에서 짐작되는 어려운 부분을 작성.\n",
    "- **문제 해결을 위한 모델 사용 방법 :** 문제에 적합한 모델을 어떻게 구성해 나갈 것인지 작성."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [배경 설명 & 요구사항 확인]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "![플라즈마 이미지](image/project/plasma.jpg)\n",
    "\n",
    "> ### 배경\n",
    "\n",
    "- 플라즈마 사용 공정 설비에서 문제 발생 시, 플라즈마 광 신호 세기가 불안정해집니다.\n",
    "\n",
    "- 제가 하는 업무는 이러한 플라즈마 이상 신호를 진단하기 위해, 플라즈마 광 신호를 실시간 측정하는 센서를 개발하고 있습니다.\n",
    "\n",
    "  ※ 플라즈마에서 방출된 photon을 electron으로 변환하여, PMT 소자를 이용해 플라즈마 광 신호 세기를 측정하는 원리입니다.\n",
    "\n",
    "> ### 요구사항\n",
    "\n",
    "- 플라즈마 불안정성으로 인한 문제 해결을 위해, 플라즈마 이상 신호를 진단하는 자동 모델을 개발하려고 합니다.\n",
    "\n",
    "- 플라즈마 이상 신호를 조기에 감지하여 엔지니어에게 알람을 제공, 플라즈마 공정 설비 안정성을 유지하는 것을 목표로 합니다.\n",
    "\n",
    "> ### 문제 범위 설정\n",
    "\n",
    "- 주요 목표는 플라즈마 광 신호를 실시간 측정하는 센서로부터 생성된 데이터를 사용하여 이상 신호 진단하는 모델을 개발하는 것입니다.\n",
    "\n",
    "> ### 문제 정의 도출\n",
    "\n",
    "- Main 문제 정의: 센서 데이터 기반으로 이상 신호를 자동으로 진단하는 자동 이상 진단 모델을 개발합니다.\n",
    "\n",
    "- Sub 문제:\n",
    "\n",
    "  1. 플라즈마 광 신호의 정상 범위 정의\n",
    "  2. 정상 범위를 벗어나는 이상 신호 정의\n",
    "  3. 정상 신호와 이상 신호 식별하는 알고리즘 개발\n",
    "  4. 개발 모델 검증, 성능 평가 지표 설정\n",
    "  5. 테스트 후 모델 조정 및 개선\n",
    "\n",
    "> ### 데이터 확보 방법\n",
    "\n",
    "- 센서 데이터 사외 반출이 안되어, 실제 신호 데이터와 최대한 유사하게 base signal을 만들고 noise와 이상 신호를 추가하여 직접 만들었습니다. (make_signal.ipynb 참고)\n",
    "\n",
    "> ### 활용 데이터 정의\n",
    "\n",
    "- 데이터 소스: 플라즈마 광 센서 데이터\n",
    "\n",
    "- 데이터 유형:\n",
    "\n",
    "   1. 정형 데이터이며, 2차원 배열 형식의 숫자 데이터\n",
    "\n",
    "- Feature:\n",
    "\n",
    "   1. Feature는 센서 데이터의 인덱스 (시간 순, 마지막 column 전까지)\n",
    "   2. 마지막 column은 target column\n",
    "   3. target column의 값은 0(정상), 1, 2, 3, 4, 5, 6(이상치/흔들림, 구간별)\n",
    "\n",
    "- 형태:\n",
    "   1. pickle 파일\n",
    "   2. 총 4가지의 다른 유형의 신호 데이터를 가짐\n",
    "   3. 각 파일은 13,600 건의 레코드를 포함\n",
    "   4. column 개수는 신호 길이에 따라 다름\n",
    "\n",
    "> ### 분석시 예상되는 어려움\n",
    "\n",
    "- 고차원 데이터: 데이터 양이 많아 리소스가 많이 투입되고 전처리 하기 어려움 (shape: 13,600 x 1,000)\n",
    "\n",
    "- 이상치 데이터 불균형: 정상 신호 1,000 개 중 이상치 1개 (0.1% 이하)\n",
    "\n",
    "- 클래스 불균형: 정상 신호와 이상 신호간 비율이 불균형 (10,000 vs 3,600)\n",
    "\n",
    "- 센서 노이즈 및 오차: 빛의 신호가 약해질 수록 상대적으로 노이즈가 커져 모델 성능 저하 가능성 존재\n",
    "\n",
    "- 모델 설명 가능성: 설득해야 하는 위치 상, 딥러닝 같이 설명 불가능한 모델은 사용이 어려움\n",
    "\n",
    "> ### 문제 해결을 위한 모델 사용 방법\n",
    "\n",
    "- 데이터 전처리\n",
    "\n",
    "   1. 데이터 탐색\n",
    "      - 데이터를 탐색하여 특징을 확인 (head, columns, unique, describe, info)\n",
    "      - 이상치나 노이즈를 확인하기 위해 데이터 분포 시각화 및 통계량 살펴봄\n",
    "   2. 데이터 결측치 처리\n",
    "      - 결측치가 존재하는 경우 해당 행은 근처 값의 평균으로 fill\n",
    "   3. 데이터 특성 선택\n",
    "      - 모두 동일한 값을 가지는 column 제거 (특정 feature가 모든 샘플에 같은 상수 값을 가짐)\n",
    "      - column끼리 비교하여 같은 값을 가지는 column들이 있다면 제거\n",
    "\n",
    "- 데이터 시각화\n",
    "\n",
    "   1. target 값 별로 어떤 종류의 이상 신호인지 확인\n",
    "      - 각 특성 분포를 시각화하여 데이터 특성 파악\n",
    "      - 정상 신호와 이상 신호 간의 클래스 비율 확인하여 불균형 있는지 확인\n",
    "   2. target column의 value_counts 이용, 정상치와 이상치 비율 확인\n",
    "\n",
    "- 모델 선택과 학습\n",
    "\n",
    "   1. 모델 선택\n",
    "      - 설명 불가능한 딥러닝 모델 보다는 설명할 수 있는 머신러닝 모델을 우선 고려\n",
    "      - 어떠어떠한, 의사결정 등\n",
    "   2. 모델 학습\n",
    "      - 데이터를 학습, 검증, 테스트 셋으로 나누고 모델 학습\n",
    "\n",
    "- 모델 평가와 해석\n",
    "\n",
    "   1. 성능 평가\n",
    "      - 모델의 성능을 평가하기 위해 정확도, 정밀도, 재현율 등의 평가 지표를 사용\n",
    "   2. 모델 해석\n",
    "      - 모델의 의사 결정 과정을 해석하기 위해 모델의 특성 중요도를 확인하고, SHAP(SHapley Additive exPlanations) 등의 기법을 사용하여 모델의 동작을 설명\n",
    "\n",
    "- 모델 최적화와 향상\n",
    "\n",
    "   1. 모델 개선\n",
    "      - 모델 별 하이퍼 파라미터 튜닝을 통한 모델 성능 최적화\n",
    "   2. 설명 가능성 강화\n",
    "      - 설명 불가능한 딥러닝 모델 보다는 설명할 수 있는 머신러닝 모델을 우선 고려\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <준비 가이드>\n",
    "#### 데이터 수집 및 로드\n",
    "- 문제 해결에 필요한 데이터를 수집 및 로드.\n",
    "\n",
    "#### 데이터 전처리\n",
    "- 결측치 처리, 이상치 처리, feature scale 등 데이터 정제.\n",
    "- 데이터의 품질을 향상시키는 것이 중요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [데이터 수집 및 로드]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사내 DB에서 SQL 쿼리를 사용하여 고객 구매 이력, 상품 정보, 고객 행동 로그 데이터를 추출합니다.\n",
    "데이터는 CSV 형식으로 저장되며, Python의 pandas 라이브러리를 사용하여 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_signal1 = pd.read_csv('./data/signal/data_1.pkl')\n",
    "# df_signal2 = pd.read_csv('./data/signal/data_2.pkl')\n",
    "# df_signal3 = pd.read_csv('./data/signal/data_3.pkl')\n",
    "# df_signal5 = pd.read_csv('./data/signal/data_5.pkl')\n",
    "\n",
    "df = pd.read_pickle('./data/signal/data_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [데이터 전처리]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불러온 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_count(df):\n",
    "    null_sum = df.isnull().sum()\n",
    "    return len(null_sum[null_sum!=0])\n",
    "\n",
    "check_null_count(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모두 동일한 값을 가지는 column 제거 (특정 feature가 모든 샘플에 같은 상수 값을 가짐)\n",
    "- nunique 함수를 이용해 nunique 값이 1이면 target과 상관 없는 변수이니 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_uniform_column(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == 1:\n",
    "            print(col)\n",
    "\n",
    "check_uniform_column(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "column끼리 비교하여 같은 값을 가지는 column들이 있다면 제거\n",
    "\n",
    "- pd.util.hash_pandas_object 함수를 이용, 해당 column 값을 해쉬로 만들어 column_hashes에 저장\n",
    "- column_hashes에 저장된 값이 또 나오면 columns_to_remove 변수에 추가해서 나중에 한 번에 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_columns(df):\n",
    "    column_hashes = {}\n",
    "    columns_to_remove = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        current_column_hash = pd.util.hash_pandas_object(df[column], index=False).sum()\n",
    "\n",
    "        if current_column_hash in column_hashes.values():\n",
    "            columns_to_remove.append(column)\n",
    "        else:\n",
    "            column_hashes[column] = current_column_hash\n",
    "\n",
    "    if columns_to_remove:\n",
    "        df.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "    print(\"제거된 열:\", columns_to_remove)\n",
    "\n",
    "check_duplicate_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target (0, 1, 2, 3, 4, 5, 6) 별로 5개씩 sampling 해서 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "fig, axes = plt.subplots(len(df['target'].unique()), num_samples, figsize=(15, 10), sharex='col')\n",
    "for i in [0, 1, 2, 3, 4, 5, 6]:\n",
    "    df_sub = df[df['target'] == i].sample(num_samples)\n",
    "    for j in range(num_samples):\n",
    "        axes[i, j].plot(df_sub.iloc[j, :-1].values)\n",
    "        axes[i, j].set_title(f'target: {i}, sample: {j+1}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target (0, 1, 2, 3, 4, 5, 6) 별로 count 개수 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "target_value_counts = df['target'].value_counts()\n",
    "print(target_value_counts)\n",
    "plt.pie(target_value_counts, labels=target_value_counts.index, explode=([0.1]*len(target_value_counts)), autopct='%1.1f%%', colors=cm.Set3.colors, startangle=90, counterclock=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- target 0 : 정상 신호 (10,000개, 73.5%)\n",
    "- target 1, 2, 3: 불규칙적인 튀는 신호 (구간 별, 각 600개, 4.4%)\n",
    "- target 4, 5, 6: 노이즈가 섞인 지속적인 신호 (구간 별, 각 600개, 4.4%)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가설 검정 해볼까?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result_classifier(y, pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    accuracy = accuracy_score(y, pred)\n",
    "    precision = precision_score(y, pred, average='macro')\n",
    "    recall = recall_score(y, pred, average='macro')\n",
    "    f1 = f1_score(y, pred, average='macro')\n",
    "    print(f'accuracy: {accuracy:.6f}, precision: {precision:.6f}, recall: {recall:.6f}, f1_score: {f1:.6f}')\n",
    "    print(confusion_matrix(y, pred))\n",
    "    print(classification_report(y, pred))\n",
    "    \n",
    "def print_result_regressor(y, pred):\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    print(f'mse: {mean_squared_error(y, pred):.6f}, r2_score: {r2_score(y, pred):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.2/0.8, random_state=0)\n",
    "\n",
    "print(f'X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}')\n",
    "print(f'y_train: {y_train.shape}, y_val: {y_val.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 혼자 Test 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_pickle('./data/signal/data_1.pkl')\n",
    "\n",
    "def print_result_classifier(y, pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    accuracy = accuracy_score(y, pred)\n",
    "    precision = precision_score(y, pred, average='macro')\n",
    "    recall = recall_score(y, pred, average='macro')\n",
    "    f1 = f1_score(y, pred, average='macro')\n",
    "    print(f'accuracy: {accuracy:.6f}, precision: {precision:.6f}, recall: {recall:.6f}, f1_score: {f1:.6f}')\n",
    "    print(confusion_matrix(y, pred))\n",
    "    print(classification_report(y, pred))\n",
    "    \n",
    "def print_result_regressor(y, pred):\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    print(f'mse: {mean_squared_error(y, pred):.6f}, r2_score: {r2_score(y, pred):.6f}')\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.2/0.8, random_state=0)\n",
    "\n",
    "print(f'X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}')\n",
    "print(f'y_train: {y_train.shape}, y_val: {y_val.shape}, y_test: {y_test.shape}')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1초\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=2)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1초\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty='l2')\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1초\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5분\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1분 30초\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1분 30초\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('logistic_regression', LogisticRegression()),\n",
    "        ('svc', SVC(kernel='linear')),\n",
    "        ('gaussianNB', GaussianNB())\n",
    "    ],\n",
    "    voting='hard',\n",
    "    weights=[1, 1, 1]\n",
    ")\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5초\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=2,\n",
    "                             random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4초\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = BaggingClassifier(estimator=GaussianNB(),\n",
    "                        n_estimators=10,\n",
    "                        random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1분\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18분\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(max_depth=2, learning_rate=0.1, random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2분 30초\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "clf1 = SVC(kernel='linear', random_state=1)\n",
    "clf2 = GaussianNB()\n",
    "clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', clf1),\n",
    "        ('gnb', clf2)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression())\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3초\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_tn_lda = clf.transform(X_train_scaled)\n",
    "X_te_lda = clf.transform(X_test_scaled)\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_tn_lda, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_te_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4초\n",
    "from sklearn.cluster import KMeans\n",
    "clf = KMeans(n_clusters=7, init='random', n_init=10, max_iter=100, random_state=0)\n",
    "clf.fit(X)\n",
    "pred = clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_counts = pd.DataFrame(pred).value_counts()\n",
    "plt.pie(x=pred_val_counts, labels=pred_val_counts.index, autopct='%1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14초\n",
    "from sklearn.cluster import DBSCAN\n",
    "clf = DBSCAN(eps=0.2)\n",
    "clf.fit(X)\n",
    "pred = clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_counts = pd.DataFrame(pred).value_counts()\n",
    "plt.pie(x=pred_val_counts, labels=pred_val_counts.index, autopct='%1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35초\n",
    "from sklearn.mixture import GaussianMixture\n",
    "clf = GaussianMixture(n_components=7, random_state=0)\n",
    "clf.fit(X)\n",
    "pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_counts = pd.DataFrame(pred).value_counts()\n",
    "plt.pie(x=pred_val_counts, labels=pred_val_counts.index, autopct='%1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2초\n",
    "from sklearn.svm import OneClassSVM\n",
    "clf = OneClassSVM(gamma='auto', nu=0.01)\n",
    "clf.fit(X)\n",
    "pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_counts = pd.DataFrame(pred).value_counts()\n",
    "plt.pie(x=pred_val_counts, labels=pred_val_counts.index, autopct='%1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4초\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "clf = LocalOutlierFactor(n_neighbors=7)\n",
    "pred = clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_counts = pd.DataFrame(pred).value_counts()\n",
    "plt.pie(x=pred_val_counts, labels=pred_val_counts.index, autopct='%1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1초\n",
    "from sklearn.ensemble import IsolationForest\n",
    "clf = IsolationForest(random_state=0)\n",
    "clf.fit(X)\n",
    "pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_counts = pd.DataFrame(pred).value_counts()\n",
    "plt.pie(x=pred_val_counts, labels=pred_val_counts.index, autopct='%1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "clf = PCA(n_components=7)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "X_tn_pca = clf.transform(X_train_scaled)\n",
    "X_te_pca = clf.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 혼자 Test 해보기 끝\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from scipy import stats\n",
    "# import numpy as np\n",
    "\n",
    "# # 결측치 처리\n",
    "# customer_purchase.fillna(customer_purchase.mean(), inplace=True)\n",
    "\n",
    "# # 이상치 처리\n",
    "# Q1 = customer_purchase.quantile(0.25)\n",
    "# Q3 = customer_purchase.quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "# customer_purchase = customer_purchase[~((customer_purchase < (Q1 - 1.5 * IQR)) |(customer_purchase > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# # 스케일링\n",
    "# scaler = StandardScaler()\n",
    "# customer_purchase_scaled = scaler.fit_transform(customer_purchase.select_dtypes(include=[np.number]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <EDA 가이드>\n",
    "- 데이터의 구조, 패턴, 이상치, 상관 관계 등을 파악\n",
    "- 시각화 도구를 사용하여 데이터를 분석\n",
    "- 모델의 성능을 향상시키기 위해 피처를 선택, 변환, 생성\n",
    "- 데이터의 정보를 최대한 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matplotlib과 seaborn 라이브러리를 사용하여 데이터의 분포, 상관 관계를 시각화합니다.\n",
    "고객의 구매 패턴, 상품 카테고리별 판매량 등 주요 인사이트를 도출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # 고객별 구매 빈도 시각화\n",
    "# customer_purchase_frequency = customer_purchase['customer_id'].value_counts()\n",
    "# sns.histplot(customer_purchase_frequency, kde=True)\n",
    "# plt.title('Customer Purchase Frequency')\n",
    "# plt.xlabel('Number of Purchases')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# # 상품 카테고리별 판매량\n",
    "# sns.countplot(data=product_info, x='category')\n",
    "# plt.title('Sales by Product Category')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.ylabel('Number of Sales')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 시각화 결과를 통해 다음과 같은 패턴을 확인할 수 있었습니다.\n",
    "\n",
    "`bill_length_mm`과 `bill_depth_mm`로 Scatter plot를 그려볼 시 각 종별로 일부 데이터를 제외하고 군집이 잘 형성됨을 확인하였습니다.\n",
    "\n",
    "- **Adelie:** 다른 펭귄 보다 bill_length가 작고, Chinstrap 펭귄과  bill_depth가 비슷하게 분포한다.\n",
    "- **Chinstrap:** Adelie 펭귄 보다 bill_length가 크고, Gentoo 펭귄과 bill_depth가 비슷하게 분포한다.\n",
    "- **Gentoo:**  Chinstrap 펭귄과 bill_length가 비슷하고, 다른 펭귄 보다 bill_depth가 작게 분포한다.\n",
    "\n",
    "\n",
    "종과 상관없이 수컷 팽귄의 `bill_length`와 `bill_depth`이 암컷보다 선형적으로 커진다는 것을 확인할 수 있었으며,\n",
    "<br>`body_mass`를 통해 Gentoo 펭귄이 다른 종보다 큰 몸집을 갖는다는 것을 확인하였습니다.\n",
    "\n",
    "이 내용을 기반으로 종별 bill_length, bill_depth 등 각 컬럼의 Number Summary를 확인 하고자 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <모델 가이드>\n",
    "- 문제에 적합한 머신러닝 알고리즘을 선택\n",
    "- 모델을 훈련하고 최적화\n",
    "- 모델의 성능을 평가(정확도, 정밀도, 재현율, F1 점수 등 다양한 지표를 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "고객의 구매 이력을 기반으로 하는 협업 필터링 모델을 구현합니다.\n",
    "scikit-learn 라이브러리를 사용하여 훈련 데이터와 테스트 데이터를 분할하고, 모델을 훈련 및 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # 데이터 준비\n",
    "# X = customer_behavior.drop('purchase', axis=1)\n",
    "# y = customer_behavior['purchase']\n",
    "\n",
    "# # 데이터 분할\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 모델 훈련 및 평가\n",
    "# model = RandomForestClassifier(n_estimators=100000000, random_state=420000000)\n",
    "# model.fit(X_train, y_train)\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **결론**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <결론 가이드>\n",
    "- 처음 정의한 문제 해결\n",
    "- 문제와 관련된 새로운 사실\n",
    "- 기존 업무 효율화\n",
    "- 데이터셋에 대한 인사이트\n",
    "- 등등\n",
    "\n",
    "여러 관점에서 프로젝트의 결과를 정리하여 최종 결론을 논리적으로 서술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추천 시스템을 통해 구매 전환율이 예상보다 X% 향상될 것으로 예측합니다.\n",
    "\n",
    "추가 데이터 수집 및 다른 알고리즘의 적용 가능성에 대해서도 탐색합니다.\n",
    "\n",
    "더 정밀한 타겟팅과 개인화된 추천을 위해, 향후에는 고급 기법들을 도입할 계획입니다. 예를 들어, 딥러닝 기반의 추천 시스템이나, 시간에 따라 변화하는 고객의 선호를 반영할 수 있는 동적 모델링 방법들입니다.\n",
    "\n",
    "이 프로젝트를 통해 얻은 인사이트와 모델은 실제 업무 프로세스에 통합될 예정입니다. 예를 들어, 마케팅 캠페인의 타겟팅 전략 수립, 재고 관리 및 물류 계획의 최적화, 신제품 개발의 방향성 결정 등 다양한 분야에서 활용될 수 있습니다.\n",
    "또한, 프로젝트 결과는 내부 리포트와 워크숍을 통해 공유되어, 조직 전반의 데이터 기반 의사결정 문화를 강화하는 데 기여할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_11_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
