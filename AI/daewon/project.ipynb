{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 삼성 INTENSIVE 현업 PROJECT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **기획**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [배경 설명 & 요구사항 확인]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![플라즈마 이미지](image/project/plasma.jpg)\n",
    "\n",
    "> ### 배경\n",
    "- 플라즈마 사용 공정 설비에서 문제 발생할 경우, 플라즈마 광 신호 세기가 불안정해집니다.\n",
    "- 제가 하는 업무는 이러한 플라즈마 이상 신호를 진단하기 위해, 플라즈마 광 신호를 실시간 측정하는 센서를 개발하고 있습니다.\n",
    "\n",
    "  ※ 플라즈마에서 방출된 photon을 electron으로 변환하여, PMT 소자를 이용해 플라즈마 광 신호 세기를 측정하는 원리입니다.\n",
    "\n",
    "> ### 요구사항\n",
    "- 플라즈마 광 신호 데이터 양이 너무 많아 분석하는데 시간이 오래 걸립니다. (50,000 points/s)\n",
    "- 플라즈마 이상 신호를 자동으로 진단하는 모델을 개발해서 이상 신호를 조기에 감지하는 것을 목표로 합니다.\n",
    "- target 이상 신호는 1 point 튀는 outlier 신호와 불안정하게 진동하는 수십 points 신호입니다.\n",
    "- 현재 과정에서의 목표는 설비 별로 발생하는 이상 신호를 진단하는 개별 모델을 만드는 것입니다.\n",
    "- 최종적으로 엔지니어에게 알람을 제공하는 시스템 구축해, 플라즈마 공정 설비 안정성을 유지하는 것을 목표로 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제 범위 설정]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주요 목표는 플라즈마 광 신호를 실시간 측정하는 센서로부터 생성된 데이터를 사용하여 이상 신호 진단하는 모델을 개발하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제 정의 도출]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 센서 데이터 기반으로 이상 신호를 자동으로 진단하는 자동 이상 진단 모델을 개발합니다.\n",
    "  1. 플라즈마 광 신호의 정상 범위 정의\n",
    "  2. 정상 범위를 벗어나는 이상 신호 정의\n",
    "  3. 정상 신호와 이상 신호 식별하는 모델 개발\n",
    "  4. 개발 모델 검증, 성능 평가 지표 설정\n",
    "  5. 테스트 후 모델 조정 및 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [데이터 확보 방법]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 센서 데이터 사외 반출이 안 되어 직접 제작했습니다.\n",
    "- 실제 신호 데이터와 최대한 유사하게 base signal을 만들고 noise와 이상 신호를 추가하여 직접 만들었습니다.\n",
    "\n",
    "   ※ 실제 현업에서 사용하는 데이터의 양이 너무 많아 도저히 만들 수 없어 최대한 많은 포인트로 제작했습니다. (avg 1,000 points/signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [활용 데이터 정의]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 소스: 플라즈마 광 센서 데이터\n",
    "- 데이터 유형:\n",
    "   1. 정형 데이터이며, 2차원 배열 형식의 숫자 데이터\n",
    "- Feature:\n",
    "   1. Feature는 센서 데이터의 인덱스 (시간 순, 마지막 column 전까지)\n",
    "   2. 마지막 column은 target column\n",
    "   3. target column의 값은 0(정상), 1, 2, 3, 4, 5, 6 (이상치/흔들림, 구간별)\n",
    "- 형태:\n",
    "   1. pickle 파일\n",
    "   2. 총 4가지의 다른 유형의 신호 데이터를 가짐\n",
    "   3. 각 파일은 13,600 건의 레코드를 포함\n",
    "   4. column 개수는 신호 길이에 따라 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [분석시 예상되는 어려움]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 고차원 데이터: 데이터 양이 많아 리소스가 많이 투입되고 전처리 하기 어려움 (1 파일의 shape: 13,600 x 1,000, 4개의 파일)\n",
    "- 이상치 데이터 불균형: 정상 신호 1,000 개 중 이상치 1개 (0.1% 이하)\n",
    "- 클래스 불균형: 정상 신호와 이상 신호간 비율이 불균형 (10,000 vs 3,600)\n",
    "- 센서 노이즈 및 오차: 빛의 신호가 약해질 수록 상대적으로 노이즈가 커져 모델 성능 저하 가능성 존재\n",
    "- 모델 설명 가능성: 상사를 설득해야 하는 업무 특성 상, 딥러닝 같이 설명 불가능한 모델은 사용이 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제 해결을 위한 모델 사용 방법]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 전처리\n",
    "   1. 데이터 전처리\n",
    "      - 결측치가 존재하는 경우 해당 행은 제거\n",
    "      - 모두 동일한 값을 가지는 column 제거 (특정 feature가 모든 샘플에 같은 상수 값을 가짐)\n",
    "      - column끼리 비교하여 같은 값을 가지는 column들이 있다면 제거\n",
    "\n",
    "- 데이터 시각화\n",
    "   1. 데이터 탐색\n",
    "      - 데이터 구조를 파악하여 특징 확인 (head, columns, unique, describe, info)\n",
    "      - 이상치나 노이즈를 확인하기 위해 데이터 분포 시각화 및 기초 통계량 확인\n",
    "      - target 값 별로 어떤 종류의 이상 신호인지 확인\n",
    "      - 각 특성 분포를 시각화하여 데이터 특성 파악\n",
    "      - 정상 신호와 이상 신호 간의 클래스 비율 확인하여 불균형 있는지 확인\n",
    "      - 신호 한 개의 데이터에서 이상 신호 데이터 비율 확인 (비정상/정상)\n",
    "\n",
    "- 모델 선택과 학습\n",
    "   1. Feature Scaling\n",
    "      - Standardization\n",
    "   2. 모델 선택\n",
    "      - 설명 불가능한 딥러닝 모델 보다는 설명할 수 있는 머신러닝 모델을 우선 고려\n",
    "      - target 데이터가 있기 때문에 분류해주는 머신러닝 지도학습 모델 평가 (강사님이 다 돌려보래서 배운 분류 모델 전부 돌릴 예정)\n",
    "         1. KNN\n",
    "         2. Logistic Regression\n",
    "         3. Naive Bayes\n",
    "         4. Decision Tree\n",
    "         5. Support Vector Machine\n",
    "         6. Random Forest\n",
    "         7. AdaBoost\n",
    "         8. GradientBoosting\n",
    "         9. LDA\n",
    "      - 위에서 나온 결과를 토대로 앙상블 모델도 평가\n",
    "         1. Voting\n",
    "         2. Bagging\n",
    "         3. Stacking\n",
    "      - 파일 용량이 커, 학습 시간이 오래 걸리면 PCA나 LDA를 이용해 차원 축소 후 모델 적용\n",
    "   3. 모델 학습\n",
    "      - 데이터를 학습, 검증, 테스트 셋으로 나누어서 학습\n",
    "\n",
    "- 모델 평가와 해석\n",
    "   1. 성능 평가\n",
    "      - 모델의 성능을 평가하기 위해 accuracy, prcision, recall, f1 score, confusion matrix 등을 사용\n",
    "   2. 모델 해석\n",
    "      - 모델의 의사 결정 과정을 해석하기 위해 모델의 특성 중요도를 확인하고, SHAP(SHapley Additive exPlanations) 등의 기법을 사용하여 모델의 동작을 설명\n",
    "\n",
    "- 모델 최적화와 향상\n",
    "   1. 모델 개선\n",
    "      - Cross validation을 이용하여 모델 평가\n",
    "      - GridSearchCV를 이용하여 모델 별 하이퍼 파라미터 튜닝을 통한 모델 성능 최적화\n",
    "      - 불필요한 특성 탐색 후, 제거\n",
    "      - 앙상블 기법 사용해서 최적화 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **내용이 너무 길어지는 관계로 총 4개의 파일 중 첫 번째 파일에 대한 전체 과정을 진행한 후**\n",
    "\n",
    "### **나머지 세 개 파일에 대해서는 모델 평가 결과만 공유드리겠습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [데이터 수집 및 로드]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사내 센서 저장소에서 신호 데이터를 추출합니다.\n",
    "- 데이터는 tdms 형식으로 저장되며, python tdms 라이브러리를 사용해서 로드합니다.\n",
    "\n",
    "   ※ tdms는 National Instrumnet 독자 포맷으로 적용이 어려워, 여기서는 편의상 빠른 속도의 pickle을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_signal1 = pd.read_pickle('./data/signal/data_1.pkl')\n",
    "# df_signal2 = pd.read_pickle('./data/signal/data_2.pkl')\n",
    "# df_signal3 = pd.read_pickle('./data/signal/data_3.pkl')\n",
    "# df_signal5 = pd.read_pickle('./data/signal/data_5.pkl')\n",
    "\n",
    "df = pd.read_pickle('./data/signal/data_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [데이터 전처리]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "결측치 확인 및 확인 시, 해당 row 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_null_rows(df):\n",
    "    null_rows = df.isnull().sum(axis=1)\n",
    "    rows_with_null = null_rows[null_rows != 0].index.tolist()\n",
    "    print('제거할 행:', rows_with_null)\n",
    "    return df.drop(index=rows_with_null)\n",
    "\n",
    "df = report_null_rows(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "모두 같은 상수 값을 가지는 column 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_uniform_columns(df):\n",
    "    columns_to_remove = [col for col in df.columns if df[col].nunique() == 1]\n",
    "    print('제거된 열:', columns_to_remove)\n",
    "    return df.drop(columns=columns_to_remove)\n",
    "\n",
    "df = remove_uniform_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "column끼리 비교하여 같은 값을 가지는 column들이 있다면 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_columns(df):\n",
    "    df_T = df.T.drop_duplicates().T\n",
    "    columns_to_remove = df.columns.difference(df_T.columns)\n",
    "    df_T['target'] = df_T['target'].astype(int)\n",
    "    print('제거된 열:', columns_to_remove.tolist())\n",
    "    return df_T\n",
    "\n",
    "df = check_duplicate_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리 과정을 통해 아래 내용을 파악했습니다.\n",
    "- isnull().sum() 이용, 해당 데이터에 결측치가 없음을 확인했습니다.\n",
    "- nunique()==1 이용, 값이 전부 동일한 상수인 column이 없음을 확인했습니다. (target과 관련 없는 무의미한 feature)\n",
    "- drop_duplicates() 이용, 같은 값을 가지는 column이 없습니다. (중복되어 선형 족속인 feature)\n",
    "\n",
    "<br>\n",
    "\n",
    "가이드에는 데이터 전처리 시, 데이터 스케일링 하라고 적혀져 있지만,\n",
    "\n",
    "**저는 데이터 스케일링은 EDA를 통해 데이터 특성 파악 및 분석 후, 진행하겠습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <EDA 가이드>\n",
    "- 데이터의 구조, 패턴, 이상치, 상관 관계 등을 파악\n",
    "- 시각화 도구를 사용하여 데이터를 분석\n",
    "- 모델의 성능을 향상시키기 위해 피처를 선택, 변환, 생성\n",
    "- 데이터의 정보를 최대한 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차원 배열 형식의 숫자형 데이터이며, 마지막 column은 target\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "column은 0 ~ 971까지 순차적으로 증가하다(feature) 마지막 column은 target\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df의 행은 13,600개, 열은 973개\n",
    "feature: 972개 열이며 float64 type\n",
    "target: 1개의 열이며 int32 type\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature를 봤을 때 (앞, 뒤쪽만)\n",
    "1. 표준편차는 크게 변하지 않음\n",
    "2. feature index가 커질 수록, std 제외 다른 기초 통계량의 변화가 있는 부분이 있음\n",
    "3. 더 자세히 확인하기 위해, np.quantile 이용해 구간 별 확인\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_10perent = (np.quantile(range(0, len(df.columns)-1), q=np.linspace(0, 1, 21))).astype(int)\n",
    "df.describe()[index_10perent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20분위수로 feature를 봤을 때\n",
    "1. feature index가 커지거나 작아질때 기초통계량의 경향성이 보이지 않음\n",
    "2. (48~194 index)와 (242~388 index)는 max 값과 std 값에서 차이 발생\n",
    "3. (534~679 index) (728~825 index), (873, 922 index)에서도 max 값과 std 값에서 차이 발생\n",
    "\n",
    "→ mean 값이 비슷한데 max값의 차이가 나는 것은, outlier 신호가 발생했다고 예상됨\n",
    "\n",
    "→ mean 값이 비슷한데 std 차이가 나는 것은, std에 영향을 줄 정도로 많은 수의 평균 0에 가까운 노이즈 신호가 발생됐다고 예상됨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['target'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target은 0, 1, 2, 3, 4, 5, 6 으로 이루어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "target (0, 1, 2, 3, 4, 5, 6) 별로 어떤 종류의 신호인지 5개씩 random sampling 해서 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "unique_targets = np.sort(df['target'].unique())\n",
    "n_unique_targets = len(unique_targets)\n",
    "\n",
    "fig, axes = plt.subplots(n_unique_targets, num_samples, figsize=(15, 10))\n",
    "for idx, target in enumerate(unique_targets):\n",
    "    df_sub = df[df['target'] == target].sample(num_samples)\n",
    "    for j in range(num_samples):\n",
    "        ax = axes[idx, j]\n",
    "        ax.plot(df_sub.iloc[j, :-1].values)\n",
    "        ax.set_title(f'target: {int(target)}, sample: {j+1}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random sample 시각화 결과 target 별로 이상 신호가 다르고 발생 구간이 다름\n",
    "\n",
    "※ 참고: 우리가 관심을 가지는 센서 데이터의 구간이 정해져 있음\n",
    "\n",
    "| Target | 설명                                     | 구간     |\n",
    "|--------|------------------------------------------|---------|\n",
    "| 0      | 정상 신호                                 |         |\n",
    "| 1      | 1 point 튄 이상 신호                      | 1번 구간 |\n",
    "| 2      | 1 point 튄 이상 신호                      | 2번 구간 |\n",
    "| 3      | 1 point 튄 이상 신호                      | 3번 구간 |\n",
    "| 4      | 100 points 이상의 흔들리는 불안정 이상 신호 | 1번 구간 |\n",
    "| 5      | 100 points 이상의 흔들리는 불안정 이상 신호 | 2번 구간 |\n",
    "| 6      | 100 points 이상의 흔들리는 불안정 이상 신호 | 3번 구간 |\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정상 신호와 이상 신호 간의 클래스 비율 확인하여 불균형 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "target_value_counts = df['target'].value_counts()\n",
    "print(target_value_counts)\n",
    "plt.pie(target_value_counts, labels=target_value_counts.index, explode=([0.1]*len(target_value_counts)), autopct='%1.1f%%', colors=cm.Set3.colors, startangle=90, counterclock=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Target | 설명                                     | 구간     | 개수   | 비율  |\n",
    "|--------|------------------------------------------|---------|--------|-------|\n",
    "| 0      | 정상 신호                                 |         | 10,000 | 73.5% |\n",
    "| 1      | 1 point 튄 이상 신호                      | 1번 구간 | 600    | 4.4%  |\n",
    "| 2      | 1 point 튄 이상 신호                      | 2번 구간 | 600    | 4.4%  |\n",
    "| 3      | 1 point 튄 이상 신호                      | 3번 구간 | 600    | 4.4%  |\n",
    "| 4      | 100 points 이상의 흔들리는 불안정 이상 신호 | 1번 구간 | 600    | 4.4%  |\n",
    "| 5      | 100 points 이상의 흔들리는 불안정 이상 신호 | 2번 구간 | 600    | 4.4%  |\n",
    "| 6      | 100 points 이상의 흔들리는 불안정 이상 신호 | 3번 구간 | 600    | 4.4%  |\n",
    "\n",
    "정상 신호 대 이상 신호 비율 - 73.5% : 26.5%\n",
    "- 정상 신호와 이상 신호 간 클래스 비율이 적절함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "신호 한 개의 데이터에서 이상 신호 데이터 비율 확인 (비정상/정상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values = [1, 2, 3, 4, 5, 6]\n",
    "n = 1\n",
    "\n",
    "np.random.seed(0)\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, target_value in enumerate(target_values, start=1):\n",
    "    target_df = df[df['target'] == target_value].iloc[:, :-1]\n",
    "    random_indices = np.random.randint(0, min(len(target_df), 600), n)\n",
    "    target_indices = target_df.iloc[random_indices]\n",
    "\n",
    "    plt.subplot(2, 3, i)\n",
    "    for index, row in enumerate(target_indices.values):\n",
    "        plt.plot(row, label=f'index {random_indices[index]}')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(f'target {target_value}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target 별로 데이터 1개씩 sampling 해서 시각화 (plt.xlim() 이용해 확대해서 확인, 길어서 생략)\n",
    "- target 1, 2, 3 구간: 이것만 봐서 모름, 중복해서 여러개 겹친 후 판단\n",
    "- target 4 구간: 200 ~ 400 (200 points)\n",
    "- target 5 구간: 500 ~ 700 (200 points)\n",
    "- target 6 구간: 850 ~ 940 (90 points)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values = [1, 2, 3, 4, 5, 6]\n",
    "n = 30\n",
    "\n",
    "vertical_lines = {\n",
    "    1: [200, 400],\n",
    "    2: [500, 700],\n",
    "    3: [850, 940],\n",
    "    4: [200, 400],\n",
    "    5: [500, 700],\n",
    "    6: [850, 940]\n",
    "}\n",
    "\n",
    "np.random.seed(0)\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, target_value in enumerate(target_values, start=1):\n",
    "    target_df = df[df['target'] == target_value].iloc[:, :-1]\n",
    "    random_indices = np.random.randint(0, 600, n)\n",
    "    target_indices = target_df.iloc[random_indices]\n",
    "\n",
    "    plt.subplot(2, 3, i)\n",
    "    for index, row in enumerate(target_indices.values):\n",
    "        plt.plot(row)\n",
    "    for line in vertical_lines[target_value]:\n",
    "        plt.axvline(x=line, color='r', linestyle='--')\n",
    "    plt.title(f'target {target_value}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target 별로 데이터 30개씩 sampling 해서 시각화\n",
    "- target 1 구간: 200 ~ 400 (1 points)\n",
    "- target 2 구간: 500 ~ 700 (1 points)\n",
    "- target 3 구간: 850 ~ 940 (1 points)\n",
    "- target 4 구간: 200 ~ 400 (200 points)\n",
    "- target 5 구간: 500 ~ 700 (200 points)\n",
    "- target 6 구간: 850 ~ 940 (90 points)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 시각화 결과를 통해 다음과 같은 패턴을 확인할 수 있었습니다.\n",
    "\n",
    "| Target | 설명                                     | index   | 개수   | 비율  |\n",
    "|--------|------------------------------------------|---------|--------|-------|\n",
    "| 0      | 정상 신호                                 |         | 10,000 | 73.5% |\n",
    "| 1      | 1 point 튄 이상 신호                      | 200~400 | 600    | 4.4%  |\n",
    "| 2      | 1 point 튄 이상 신호                      | 500~700 | 600    | 4.4%  |\n",
    "| 3      | 1 point 튄 이상 신호                      | 850~940 | 600    | 4.4%  |\n",
    "| 4      | 200 points 흔들리는 불안정 이상 신호       | 200~400 | 600    | 4.4%  |\n",
    "| 5      | 200 points 이상의 흔들리는 불안정 이상 신호 | 500~700 | 600    | 4.4%  |\n",
    "| 6      | 90 points 이상의 흔들리는 불안정 이상 신호  | 850~940 | 600    | 4.4%  |\n",
    "\n",
    "이 내용을 기반으로 Feature scaling 진행 후, 해당 데이터에 적합한 머신러닝 모델을 개발하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 설명 불가능한 딥러닝 모델 보다는 설명할 수 있는 머신러닝 모델을 우선 고려\n",
    "- target 데이터가 있기 때문에 분류해주는 머신러닝 모델 평가 (강사님이 다 돌려보래서 배운 분류 모델 전부 돌릴 예정)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "머신러닝 모델 학습을 위한 Feature scaling\n",
    "\n",
    "**Standardization** 선정\n",
    "\n",
    "선정 근거\n",
    "\n",
    "- Standardization\n",
    "   1. 평균과 표준편차를 이용하기 떄문에, 이상 신호 값이 표준편차에 영향을 줄 정도로 많지 않는 이상 데이터 전체에 미치는 영향이 Normalization에 비해 상대적으로 적음\n",
    "\n",
    "- Normalization\n",
    "   1. 이상 신호 값이 엄청나게 크면 데이터가 전체적으로 한쪽으로 쏠림 → 따라서 정상 값들이 작은 구간 내로 몰릴 것으로 예상됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result_classifier(y, pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    accuracy = accuracy_score(y, pred)\n",
    "    precision = precision_score(y, pred, average='macro')\n",
    "    recall = recall_score(y, pred, average='macro')\n",
    "    f1 = f1_score(y, pred, average='macro')\n",
    "    print(f'accuracy: {accuracy:.6f}, precision: {precision:.6f}, recall: {recall:.6f}, f1_score: {f1:.6f}')\n",
    "    print(confusion_matrix(y, pred))\n",
    "    print(classification_report(y, pred))\n",
    "    \n",
    "def print_result_regressor(y, pred):\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    print(f'mse: {mean_squared_error(y, pred):.6f}, r2_score: {r2_score(y, pred):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.2/0.8, random_state=0)\n",
    "\n",
    "print(f'X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}')\n",
    "print(f'y_train: {y_train.shape}, y_val: {y_val.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 혼자 Test 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_pickle('./data/signal/data_1.pkl')\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.2/0.8, random_state=0)\n",
    "\n",
    "print(f'X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}')\n",
    "print(f'y_train: {y_train.shape}, y_val: {y_val.shape}, y_test: {y_test.shape}')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1초\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=2)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1초\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty='l2')\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1초\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5분\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1분 30초\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1분 30초\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('logistic_regression', LogisticRegression()),\n",
    "        ('svc', SVC(kernel='linear')),\n",
    "        ('gaussianNB', GaussianNB())\n",
    "    ],\n",
    "    voting='hard',\n",
    "    weights=[1, 1, 1]\n",
    ")\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5초\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=2,\n",
    "                             random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4초\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = BaggingClassifier(estimator=GaussianNB(),\n",
    "                        n_estimators=10,\n",
    "                        random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1분\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18분\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(max_depth=2, learning_rate=0.1, random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2분 30초\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "clf1 = SVC(kernel='linear', random_state=1)\n",
    "clf2 = GaussianNB()\n",
    "clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', clf1),\n",
    "        ('gnb', clf2)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression())\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3초\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_test_scaled))\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_tn_lda = clf.transform(X_train_scaled)\n",
    "X_te_lda = clf.transform(X_test_scaled)\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_tn_lda, y_train)\n",
    "print_result_classifier(y_test, clf.predict(X_te_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4초\n",
    "from sklearn.cluster import KMeans\n",
    "clf = KMeans(n_clusters=7, init='random', n_init=10, max_iter=100, random_state=0)\n",
    "clf.fit(X)\n",
    "pred = clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_counts = pd.DataFrame(pred).value_counts()\n",
    "plt.pie(x=pred_val_counts, labels=pred_val_counts.index, autopct='%1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14초\n",
    "from sklearn.cluster import DBSCAN\n",
    "clf = DBSCAN(eps=0.2)\n",
    "clf.fit(X)\n",
    "pred = clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_counts = pd.DataFrame(pred).value_counts()\n",
    "plt.pie(x=pred_val_counts, labels=pred_val_counts.index, autopct='%1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35초\n",
    "from sklearn.mixture import GaussianMixture\n",
    "clf = GaussianMixture(n_components=7, random_state=0)\n",
    "clf.fit(X)\n",
    "pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_counts = pd.DataFrame(pred).value_counts()\n",
    "plt.pie(x=pred_val_counts, labels=pred_val_counts.index, autopct='%1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2초\n",
    "from sklearn.svm import OneClassSVM\n",
    "clf = OneClassSVM(gamma='auto', nu=0.01)\n",
    "clf.fit(X)\n",
    "pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_counts = pd.DataFrame(pred).value_counts()\n",
    "plt.pie(x=pred_val_counts, labels=pred_val_counts.index, autopct='%1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4초\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "clf = LocalOutlierFactor(n_neighbors=7)\n",
    "pred = clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_counts = pd.DataFrame(pred).value_counts()\n",
    "plt.pie(x=pred_val_counts, labels=pred_val_counts.index, autopct='%1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1초\n",
    "from sklearn.ensemble import IsolationForest\n",
    "clf = IsolationForest(random_state=0)\n",
    "clf.fit(X)\n",
    "pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_counts = pd.DataFrame(pred).value_counts()\n",
    "plt.pie(x=pred_val_counts, labels=pred_val_counts.index, autopct='%1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "clf = PCA(n_components=7)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "X_tn_pca = clf.transform(X_train_scaled)\n",
    "X_te_pca = clf.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 혼자 Test 해보기 끝\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **결론**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <결론 가이드>\n",
    "- 처음 정의한 문제 해결\n",
    "- 문제와 관련된 새로운 사실\n",
    "- 기존 업무 효율화\n",
    "- 데이터셋에 대한 인사이트\n",
    "- 등등\n",
    "\n",
    "여러 관점에서 프로젝트의 결과를 정리하여 최종 결론을 논리적으로 서술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추천 시스템을 통해 구매 전환율이 예상보다 X% 향상될 것으로 예측합니다.\n",
    "\n",
    "추가 데이터 수집 및 다른 알고리즘의 적용 가능성에 대해서도 탐색합니다.\n",
    "\n",
    "더 정밀한 타겟팅과 개인화된 추천을 위해, 향후에는 고급 기법들을 도입할 계획입니다. 예를 들어, 딥러닝 기반의 추천 시스템이나, 시간에 따라 변화하는 고객의 선호를 반영할 수 있는 동적 모델링 방법들입니다.\n",
    "\n",
    "이 프로젝트를 통해 얻은 인사이트와 모델은 실제 업무 프로세스에 통합될 예정입니다. 예를 들어, 마케팅 캠페인의 타겟팅 전략 수립, 재고 관리 및 물류 계획의 최적화, 신제품 개발의 방향성 결정 등 다양한 분야에서 활용될 수 있습니다.\n",
    "또한, 프로젝트 결과는 내부 리포트와 워크숍을 통해 공유되어, 조직 전반의 데이터 기반 의사결정 문화를 강화하는 데 기여할 것입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_11_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
