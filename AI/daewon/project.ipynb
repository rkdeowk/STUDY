{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 삼성 INTENSIVE 현업 PROJECT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **기획**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [배경 설명 & 요구사항 확인]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![플라즈마 이미지](image/project/plasma.jpg)\n",
    "\n",
    "> ### 배경\n",
    "- 플라즈마 사용 공정 설비에서 문제 발생할 경우, 플라즈마 광 신호 세기가 불안정해집니다.\n",
    "- 제가 하는 업무는 이러한 플라즈마 이상 신호를 진단하기 위해, 플라즈마 광 신호를 실시간 측정하는 센서를 개발하고 있습니다.\n",
    "\n",
    "  ※ 플라즈마에서 방출된 photon을 electron으로 변환하여, PMT 소자를 이용해 플라즈마 광 신호 세기를 측정하는 원리입니다.\n",
    "\n",
    "> ### 요구사항\n",
    "- 플라즈마 광 신호 데이터 양이 너무 많아 분석하는데 시간이 오래 걸립니다. (50,000 points/s)\n",
    "- 플라즈마 이상 신호를 자동으로 진단하는 모델을 개발해서 이상 신호를 조기에 감지하는 것을 목표로 합니다.\n",
    "- target 이상 신호는 1 point 튀는 outlier 신호와 불안정하게 진동하는 수십 points 신호입니다.\n",
    "- 현재 과정에서의 목표는 설비 별로 발생하는 이상 신호를 진단하는 개별 모델을 만드는 것입니다.\n",
    "- 최종적으로 엔지니어에게 알람을 제공하는 시스템 구축해, 플라즈마 공정 설비 안정성을 유지하는 것을 목표로 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제 범위 설정]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주요 목표는 플라즈마 광 신호를 실시간 측정하는 센서로부터 생성된 데이터를 사용하여 이상 신호 진단하는 모델을 개발하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제 정의 도출]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 센서 데이터 기반으로 이상 신호를 자동으로 진단하는 자동 이상 진단 모델을 개발합니다.\n",
    "  1. 플라즈마 광 신호의 정상 범위 정의\n",
    "  2. 정상 범위를 벗어나는 이상 신호 정의\n",
    "  3. 정상 신호와 이상 신호 식별하는 모델 개발\n",
    "  4. 개발 모델 검증, 성능 평가 지표 설정\n",
    "  5. 테스트 후 모델 조정 및 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [데이터 확보 방법]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 센서 데이터 사외 반출이 안 되어 직접 제작했습니다.\n",
    "- 실제 신호 데이터와 최대한 유사하게 base signal을 만들고 noise와 이상 신호를 추가하여 직접 만들었습니다.\n",
    "\n",
    "   ※ 실제 현업에서 사용하는 데이터의 양이 너무 많아 도저히 만들 수 없어 최대한 많은 포인트로 제작했습니다. (avg 1,000 points/signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [활용 데이터 정의]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 소스: 플라즈마 광 센서 데이터\n",
    "- 데이터 유형:\n",
    "   1. 정형 데이터이며, 2차원 배열 형식의 숫자 데이터\n",
    "- Feature:\n",
    "   1. Feature는 센서 데이터의 인덱스 (시간 순, 마지막 column 전까지)\n",
    "   2. 마지막 column은 target column\n",
    "   3. target column의 값은 0(정상), 1, 2, 3, 4, 5, 6 (이상치/흔들림, 구간별)\n",
    "- 형태:\n",
    "   1. pickle 파일\n",
    "   2. 총 4가지의 다른 유형의 신호 데이터를 가짐\n",
    "   3. 각 파일은 13,600 건의 레코드를 포함\n",
    "   4. column 개수는 신호 길이에 따라 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [분석시 예상되는 어려움]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 고차원 데이터: 데이터 양이 많아 리소스가 많이 투입되고 전처리 하기 어려움 (1 파일의 shape: 13,600 x 1,000, 4개의 파일)\n",
    "- 이상치 데이터 불균형: 정상 신호 1,000 개 중 이상치 1개 (0.1% 이하)\n",
    "- 클래스 불균형: 정상 신호와 이상 신호간 비율이 불균형 (10,000 vs 3,600)\n",
    "- 센서 노이즈 및 오차: 빛의 신호가 약해질 수록 상대적으로 노이즈가 커져 모델 성능 저하 가능성 존재\n",
    "- 모델 설명 가능성: 상사를 설득해야 하는 업무 특성 상, 딥러닝 같이 설명 불가능한 모델은 사용이 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제 해결을 위한 모델 사용 방법]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 전처리\n",
    "   1. 데이터 전처리\n",
    "      - 결측치가 존재하는 경우 해당 행은 제거\n",
    "      - 모두 동일한 값을 가지는 column 제거 (특정 feature가 모든 샘플에 같은 상수 값을 가짐)\n",
    "      - column끼리 비교하여 같은 값을 가지는 column들이 있다면 제거\n",
    "\n",
    "- 데이터 시각화\n",
    "   1. 데이터 탐색\n",
    "      - 데이터 구조를 파악하여 특징 확인 (head, columns, unique, describe, info)\n",
    "      - 이상치나 노이즈를 확인하기 위해 데이터 분포 시각화 및 기초 통계량 확인\n",
    "      - target 값 별로 어떤 종류의 이상 신호인지 확인\n",
    "      - 각 특성 분포를 시각화하여 데이터 특성 파악\n",
    "      - 정상 신호와 이상 신호 간의 클래스 비율 확인하여 불균형 있는지 확인\n",
    "      - 신호 한 개의 데이터에서 이상 신호 데이터 비율 확인 (비정상/정상)\n",
    "\n",
    "- 모델 선택과 학습\n",
    "   1. Feature Scaling\n",
    "      - Standardization\n",
    "   2. 모델 선택\n",
    "      - 설명 불가능한 딥러닝 모델 보다는 설명할 수 있는 머신러닝 모델을 우선 고려\n",
    "      - target 데이터가 있기 때문에 분류해주는 머신러닝 지도학습 모델 평가 (강사님이 다 돌려보래서 배운 분류 모델 전부 돌릴 예정)\n",
    "         1. KNN\n",
    "         2. Logistic Regression\n",
    "         3. Naive Bayes\n",
    "         4. Decision Tree\n",
    "         5. Support Vector Machine\n",
    "         6. Random Forest\n",
    "         7. AdaBoost\n",
    "         8. GradientBoosting\n",
    "         9. LDA\n",
    "      - 위에서 나온 결과를 토대로 앙상블 모델도 평가\n",
    "         1. Voting\n",
    "         2. Bagging\n",
    "         3. Stacking\n",
    "   3. 모델 학습\n",
    "      - 데이터를 학습, 검증, 테스트 셋으로 나누어서 학습\n",
    "      - 파일 용량이 커, 학습 시간이 오래 걸리면 PCA나 LDA를 이용해 차원 축소 후 모델 적용\n",
    "\n",
    "- 모델 평가와 해석\n",
    "   1. 성능 평가\n",
    "      - 모델의 성능을 평가하기 위해 accuracy, prcision, recall, f1 score, confusion matrix 등을 사용\n",
    "   2. 모델 해석\n",
    "      - 모델의 의사 결정 과정을 해석하기 위해 모델의 특성 중요도를 확인하고, SHAP(SHapley Additive exPlanations) 등의 기법을 사용하여 모델의 동작을 설명\n",
    "\n",
    "- 모델 최적화와 향상\n",
    "   1. 모델 개선\n",
    "      - Cross validation을 이용하여 모델 평가\n",
    "      - GridSearchCV를 이용하여 모델 별 하이퍼 파라미터 튜닝을 통한 모델 성능 최적화\n",
    "      - 불필요한 특성 탐색 후, 제거\n",
    "      - 50% 이상의 성능을 내는 모델들을 조합하여 앙상블 기법 사용해서 최적화 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **내용이 너무 길어지는 관계로 총 4개의 파일 중 첫 번째 파일에 대한 전체 과정을 진행한 후**\n",
    "\n",
    "### **나머지 세 개 파일에 대해서는 모델 평가 결과만 공유드리겠습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [데이터 수집 및 로드]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사내 센서 저장소에서 신호 데이터를 추출합니다.\n",
    "- 데이터는 tdms 형식으로 저장되며, python tdms 라이브러리를 사용해서 로드합니다.\n",
    "\n",
    "   ※ tdms는 National Instrumnet 독자 포맷으로 적용이 어려워, 여기서는 편의상 빠른 속도의 pickle을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_signal1 = pd.read_pickle('./data/signal/data_1.pkl')\n",
    "# df_signal2 = pd.read_pickle('./data/signal/data_2.pkl')\n",
    "# df_signal3 = pd.read_pickle('./data/signal/data_3.pkl')\n",
    "# df_signal5 = pd.read_pickle('./data/signal/data_5.pkl')\n",
    "\n",
    "df = pd.read_pickle('./data/signal/data_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [데이터 전처리]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "결측치 확인 및 확인 시, 해당 row 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_null_rows(df):\n",
    "    null_rows = df.isnull().sum(axis=1)\n",
    "    rows_with_null = null_rows[null_rows != 0].index.tolist()\n",
    "    print('제거할 행:', rows_with_null)\n",
    "    return df.drop(index=rows_with_null)\n",
    "\n",
    "df = report_null_rows(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "모두 같은 상수 값을 가지는 column 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_uniform_columns(df):\n",
    "    columns_to_remove = [col for col in df.columns if df[col].nunique() == 1]\n",
    "    print('제거된 열:', columns_to_remove)\n",
    "    return df.drop(columns=columns_to_remove)\n",
    "\n",
    "df = remove_uniform_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "column끼리 비교하여 같은 값을 가지는 column들이 있다면 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_columns(df):\n",
    "    df_T = df.T.drop_duplicates().T\n",
    "    columns_to_remove = df.columns.difference(df_T.columns)\n",
    "    df_T['target'] = df_T['target'].astype(int)\n",
    "    print('제거된 열:', columns_to_remove.tolist())\n",
    "    return df_T\n",
    "\n",
    "df = check_duplicate_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리 과정을 통해 아래 내용을 파악했습니다.\n",
    "- isnull().sum() 이용, 해당 데이터에 결측치가 없음을 확인했습니다.\n",
    "- nunique()==1 이용, 값이 전부 동일한 상수인 column이 없음을 확인했습니다. (target과 관련 없는 무의미한 feature)\n",
    "- drop_duplicates() 이용, 같은 값을 가지는 column이 없습니다. (중복되어 선형 족속인 feature)\n",
    "\n",
    "<br>\n",
    "\n",
    "가이드에는 데이터 전처리 시, 데이터 스케일링 하라고 적혀져 있지만,\n",
    "\n",
    "**저는 데이터 스케일링은 EDA를 통해 데이터 특성 파악 및 분석 후, 진행하겠습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <EDA 가이드>\n",
    "- 데이터의 구조, 패턴, 이상치, 상관 관계 등을 파악\n",
    "- 시각화 도구를 사용하여 데이터를 분석\n",
    "- 모델의 성능을 향상시키기 위해 피처를 선택, 변환, 생성\n",
    "- 데이터의 정보를 최대한 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차원 배열 형식의 숫자형 데이터이며, 마지막 column은 target\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "column은 0 ~ 971까지 순차적으로 증가하다(feature) 마지막 column은 target\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df의 행은 13,600개, 열은 973개\n",
    "feature: 972개 열이며 float64 type\n",
    "target: 1개의 열이며 int32 type\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature를 봤을 때 (앞, 뒤쪽만)\n",
    "1. 표준편차는 크게 변하지 않음\n",
    "2. feature index가 커질 수록, std 제외 다른 기초 통계량의 변화가 있는 부분이 있음\n",
    "3. 더 자세히 확인하기 위해, np.quantile 이용해 구간 별 확인\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_10perent = (np.quantile(range(0, len(df.columns)-1), q=np.linspace(0, 1, 21))).astype(int)\n",
    "df.describe()[index_10perent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20분위수로 feature를 봤을 때\n",
    "1. feature index가 커지거나 작아질때 기초통계량의 경향성이 보이지 않음\n",
    "2. (48~194 index)와 (242~388 index)는 max 값과 std 값에서 차이 발생\n",
    "3. (534~679 index) (728~825 index), (873, 922 index)에서도 max 값과 std 값에서 차이 발생\n",
    "\n",
    "→ mean 값이 비슷한데 max값의 차이가 나는 것은, outlier 신호가 발생했다고 예상됨\n",
    "\n",
    "→ mean 값이 비슷한데 std 차이가 나는 것은, std에 영향을 줄 정도로 많은 수의 평균 0에 가까운 노이즈 신호가 발생됐다고 예상됨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['target'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target은 0, 1, 2, 3, 4, 5, 6 으로 이루어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "target (0, 1, 2, 3, 4, 5, 6) 별로 어떤 종류의 신호인지 5개씩 random sampling 해서 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "unique_targets = np.sort(df['target'].unique())\n",
    "n_unique_targets = len(unique_targets)\n",
    "\n",
    "fig, axes = plt.subplots(n_unique_targets, num_samples, figsize=(15, 10))\n",
    "for idx, target in enumerate(unique_targets):\n",
    "    df_sub = df[df['target'] == target].sample(num_samples)\n",
    "    for j in range(num_samples):\n",
    "        ax = axes[idx, j]\n",
    "        ax.plot(df_sub.iloc[j, :-1].values)\n",
    "        ax.set_title(f'target: {int(target)}, sample: {j+1}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random sample 시각화 결과 target 별로 이상 신호가 다르고 발생 구간이 다름\n",
    "\n",
    "※ 참고: 우리가 관심을 가지는 센서 데이터의 구간이 정해져 있음\n",
    "\n",
    "| Target | 설명                                     | 구간     |\n",
    "|--------|------------------------------------------|---------|\n",
    "| 0      | 정상 신호                                 |         |\n",
    "| 1      | 1 point 튄 이상 신호                      | 1번 구간 |\n",
    "| 2      | 1 point 튄 이상 신호                      | 2번 구간 |\n",
    "| 3      | 1 point 튄 이상 신호                      | 3번 구간 |\n",
    "| 4      | 100 points 이상의 흔들리는 불안정 이상 신호 | 1번 구간 |\n",
    "| 5      | 100 points 이상의 흔들리는 불안정 이상 신호 | 2번 구간 |\n",
    "| 6      | 100 points 이상의 흔들리는 불안정 이상 신호 | 3번 구간 |\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정상 신호와 이상 신호 간의 클래스 비율 확인하여 불균형 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "target_value_counts = df['target'].value_counts()\n",
    "print(target_value_counts)\n",
    "plt.pie(target_value_counts, labels=target_value_counts.index, explode=([0.1]*len(target_value_counts)), autopct='%1.1f%%', colors=cm.Set3.colors, startangle=90, counterclock=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Target | 설명                                     | 구간     | 개수   | 비율  |\n",
    "|--------|------------------------------------------|---------|--------|-------|\n",
    "| 0      | 정상 신호                                 |         | 10,000 | 73.5% |\n",
    "| 1      | 1 point 튄 이상 신호                      | 1번 구간 | 600    | 4.4%  |\n",
    "| 2      | 1 point 튄 이상 신호                      | 2번 구간 | 600    | 4.4%  |\n",
    "| 3      | 1 point 튄 이상 신호                      | 3번 구간 | 600    | 4.4%  |\n",
    "| 4      | 100 points 이상의 흔들리는 불안정 이상 신호 | 1번 구간 | 600    | 4.4%  |\n",
    "| 5      | 100 points 이상의 흔들리는 불안정 이상 신호 | 2번 구간 | 600    | 4.4%  |\n",
    "| 6      | 100 points 이상의 흔들리는 불안정 이상 신호 | 3번 구간 | 600    | 4.4%  |\n",
    "\n",
    "정상 신호 대 이상 신호 비율 - 73.5% : 26.5%\n",
    "- 정상 신호와 이상 신호 간 클래스 비율이 적절함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "신호 한 개의 데이터에서 이상 신호 데이터 비율 확인 (비정상/정상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values = [1, 2, 3, 4, 5, 6]\n",
    "n = 1\n",
    "\n",
    "np.random.seed(0)\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, target_value in enumerate(target_values, start=1):\n",
    "    target_df = df[df['target'] == target_value].iloc[:, :-1]\n",
    "    random_indices = np.random.randint(0, min(len(target_df), 600), n)\n",
    "    target_indices = target_df.iloc[random_indices]\n",
    "\n",
    "    plt.subplot(2, 3, i)\n",
    "    for index, row in enumerate(target_indices.values):\n",
    "        plt.plot(row, label=f'index {random_indices[index]}')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(f'target {target_value}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target 별로 데이터 1개씩 sampling 해서 시각화 (plt.xlim() 이용해 확대해서 확인, 길어서 생략)\n",
    "- target 1, 2, 3 구간: 이것만 봐서 모름, 중복해서 여러개 겹친 후 판단\n",
    "- target 4 구간: 200 ~ 400 (200 points)\n",
    "- target 5 구간: 500 ~ 700 (200 points)\n",
    "- target 6 구간: 850 ~ 940 (90 points)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values = [1, 2, 3, 4, 5, 6]\n",
    "n = 30\n",
    "\n",
    "vertical_lines = {\n",
    "    1: [200, 400],\n",
    "    2: [500, 700],\n",
    "    3: [850, 940],\n",
    "    4: [200, 400],\n",
    "    5: [500, 700],\n",
    "    6: [850, 940]\n",
    "}\n",
    "\n",
    "np.random.seed(0)\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, target_value in enumerate(target_values, start=1):\n",
    "    target_df = df[df['target'] == target_value].iloc[:, :-1]\n",
    "    random_indices = np.random.randint(0, 600, n)\n",
    "    target_indices = target_df.iloc[random_indices]\n",
    "\n",
    "    plt.subplot(2, 3, i)\n",
    "    for index, row in enumerate(target_indices.values):\n",
    "        plt.plot(row)\n",
    "    for line in vertical_lines[target_value]:\n",
    "        plt.axvline(x=line, color='r', linestyle='--')\n",
    "    plt.title(f'target {target_value}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target 별로 데이터 30개씩 sampling 해서 시각화\n",
    "- target 1 구간: 200 ~ 400 (1 points)\n",
    "- target 2 구간: 500 ~ 700 (1 points)\n",
    "- target 3 구간: 850 ~ 940 (1 points)\n",
    "- target 4 구간: 200 ~ 400 (200 points)\n",
    "- target 5 구간: 500 ~ 700 (200 points)\n",
    "- target 6 구간: 850 ~ 940 (90 points)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 시각화 결과를 통해 다음과 같은 패턴을 확인할 수 있었습니다.\n",
    "\n",
    "| Target | 설명                                     | index   | 개수   | 비율  |\n",
    "|--------|------------------------------------------|---------|--------|-------|\n",
    "| 0      | 정상 신호                                 |         | 10,000 | 73.5% |\n",
    "| 1      | 1 point 튄 이상 신호                      | 200~400 | 600    | 4.4%  |\n",
    "| 2      | 1 point 튄 이상 신호                      | 500~700 | 600    | 4.4%  |\n",
    "| 3      | 1 point 튄 이상 신호                      | 850~940 | 600    | 4.4%  |\n",
    "| 4      | 200 points 흔들리는 불안정 이상 신호       | 200~400 | 600    | 4.4%  |\n",
    "| 5      | 200 points 이상의 흔들리는 불안정 이상 신호 | 500~700 | 600    | 4.4%  |\n",
    "| 6      | 90 points 이상의 흔들리는 불안정 이상 신호  | 850~940 | 600    | 4.4%  |\n",
    "\n",
    "이 내용을 기반으로 Feature scaling 진행 후, 해당 데이터에 적합한 머신러닝 모델을 개발하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 설명 불가능한 딥러닝 모델 보다는 설명할 수 있는 머신러닝 모델을 우선 고려\n",
    "- target 데이터가 있기 때문에 분류해주는 머신러닝 모델 평가 (강사님이 다 돌려보래서 배운 분류 모델 전부 돌릴 예정)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 머신러닝 모델 학습을 위한 Feature scaling\n",
    "\n",
    "**Standardization** 선정\n",
    "\n",
    "선정 근거\n",
    "- Standardization\n",
    "   1. 평균과 표준편차를 이용하기 떄문에, 이상 신호 값이 표준편차에 영향을 줄 정도로 많지 않는 이상 데이터 전체에 미치는 영향이 Normalization에 비해 상대적으로 적음\n",
    "\n",
    "- Normalization\n",
    "   1. 이상 신호 값이 엄청나게 크면 데이터가 전체적으로 한쪽으로 쏠림 → 따라서 정상 값들이 작은 구간 내로 몰릴 것으로 예상됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 모델 평가 계획\n",
    "- 배운 내용 위주로 hyper parmeter를 설정 (데이터 값이 커서 속도가 매우 느림)\n",
    "- 결과가 좋으면 (StratifiedKFold + GridSearchCV)를 이용해서 hyper parameter tuning\n",
    "- 결과가 안 좋으면 hyper parameter 몇번 변경하고 값이 좋아지면 hyper parameter tuning\n",
    "- 기본 학습 소요 시간이 10분 이상 되는 모델은 차원 축소 알고리즘(PCA or LDA) 적용\n",
    "\n",
    "<br>\n",
    "\n",
    "| No. | 머신러닝 모델           | 상세                                                                                  |\n",
    "|-----|------------------------|---------------------------------------------------------------------------------------|\n",
    "| 1   | KNN                    | target 7개라 n_neighbors=7로 학습                                                      |\n",
    "| 2   | Logistic Regression    | 제약식 평가 (penalty = [l1, l2, elasticnet])                                           |\n",
    "| 3   | Naive Bayes            |                                                                                       |\n",
    "| 4   | Decision Tree          | max_depth 평가, criterion 평가 (gini, entropy)                                         |\n",
    "| 5   | Support Vector Machine | kernel 평가, C 평가                                                                    |\n",
    "| 6   | Random Forest          | max_depth 평가, criterion 평가 (gini, entropy)                                         |\n",
    "| 7   | AdaBoost               |                                                                                       |\n",
    "| 8   | GradientBoosting       | max_depth 평가, learning rate 평가                                                     |\n",
    "| 9   | LDA                    |                                                                                       |\n",
    "| 10  | Voting (앙상블)         | estimator 평가 (위 모델들 조합), voting 평가 (soft, hard), weights 평가                 |\n",
    "| 11  | Bagging (앙상블)        | estimator 평가 (위 모델들 조합), n_estimators 평가                                     |\n",
    "| 12  | Stacking (앙상블)       | estimator, final estimator 평가 (위 모델들 조합)                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "분류 결과를 보기 위한 함수 작성\n",
    "- accuracy: 전체 예측 중 올바른 예측의 비율\n",
    "- precision: 모델이 양성이라고 예측한 것 중 실제로 양성인 것의 비율\n",
    "- recall: 실제 양성 샘플 중 모델이 양성으로 올바르게 예측한 샘플의 비율\n",
    "- f1 score: 정밀도와 재현율의 조화 평균 (f1 score가 낮으면 모델의 정밀도와 재현율이 모두 낮다)\n",
    "- confusion matrix: 모델이 클래스를 어떻게 예측했는지\n",
    "- calssification report: 각 클래스별로 정밀도, 재현율, F1 점수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result_classifier(y, pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    accuracy = accuracy_score(y, pred)\n",
    "    precision = precision_score(y, pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y, pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y, pred, average='macro', zero_division=0)\n",
    "    print(f'accuracy: {accuracy:.6f}, precision: {precision:.6f}, recall: {recall:.6f}, f1_score: {f1:.6f}')\n",
    "    print(confusion_matrix(y, pred))\n",
    "    print(classification_report(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 혼자 Test 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_result_classifier(y, pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    accuracy = accuracy_score(y, pred)\n",
    "    precision = precision_score(y, pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y, pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y, pred, average='macro', zero_division=0)\n",
    "    print(f'accuracy: {accuracy:.6f}, precision: {precision:.6f}, recall: {recall:.6f}, f1_score: {f1:.6f}')\n",
    "    print(confusion_matrix(y, pred))\n",
    "    print(classification_report(y, pred, zero_division=0))\n",
    "\n",
    "def grid_search_cv(model_class, X_train, y_train, param_grid, n_splits=5):\n",
    "    from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    model = model_class(random_state=0)\n",
    "    grid_cv = GridSearchCV(model, param_grid, cv=kfold, scoring='accuracy')\n",
    "    grid_cv.fit(X_train, y_train)\n",
    "    clf = grid_cv.best_estimator_\n",
    "    grid_cv_results_df = pd.DataFrame(grid_cv.cv_results_).T\n",
    "    return clf, grid_cv_results_df\n",
    "\n",
    "df = pd.read_pickle('./data/signal/data_1.pkl')\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1. KNN\n",
    "   - target이 7개 이므로 n_nighbors=7 넣고 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=7)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN 결론 **(모델 후보 탈락)**\n",
    "- accuracy: 예측 대부분이 target 0인데, target 0의 비율이 73.5%라서, 예측이 잘못되었음\n",
    "- precision: 예측한 것 중 절반 이상이 실제 양성\n",
    "- recall: target을 거의 찾지 못함\n",
    "- f1 score: precision, recall 모두 낮다\n",
    "- confusion matrix: target을 대부분 0으로 예측\n",
    "- classification report: 성능이 매우 낮고, class 4, 5, 6은 전부 예측 못함\n",
    "- 결과 값이 좋지 않아 hyper parameter tuning pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2. Logistic Regression\n",
    "   - 제약식 평가 (penalty = [l1, l2, elasticnet])\n",
    "   > The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization with primal formulation, or no regularization. The ‘liblinear’ solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the ‘saga’ solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty='l2')\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.01)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression 결론 **(모델 후보 탈락)**\n",
    "- L1, L2, Elastinet 성능이 전부 좋지 않음\n",
    "- Elastinet > L1 > L2 성능 순\n",
    "- KNN 모델보다 조금 더 나은 수준\n",
    "- 결과 값이 좋지 않아 hyper parameter tuning pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes 결론 **(모델 후보 선정)**\n",
    "- 모든 score들이 매우 높아, 잘 예측함을 확인\n",
    "- 잘못 분류된 것을 보면 1 → 4 (23개), 2 → 5 (20개), 3 → 6 (50개), 6 → 3(3개)\n",
    "   - outlier target: 1, 3, 5 (구간 별)\n",
    "   - 흔들리는 신호 target: 2, 4, 6 (구간 별)\n",
    "   - 잘못 분류한 원인을 파악하지 못하겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "4. Decision Tree\n",
    "   - max_depth 평가, criterion 평가 (gini, entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과가 매우 좋음\n",
    "- StratifiedKFold + GridSearchCV 사용해서 hyper parameter tuning 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 50, 100],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "clf, cv_result = grid_search_cv(DecisionTreeClassifier, X_train_scaled, y_train, param_grid)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)\n",
    "cv_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree 결론 **(모델 후보 선정)**\n",
    "- 모든 score들이 매우 높아, 잘 예측함을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "5. Support Vector Machine\n",
    "   - kernel 평가, C 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과가 매우 좋음\n",
    "- StratifiedKFold + GridSearchCV 사용해서 hyper parameter tuning 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = {\n",
    "    'kernel': ['rbf', 'linear', 'sigmoid', 'poly'],\n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "clf, cv_result = grid_search_cv(SVC, X_train_scaled, y_train, param_grid)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)\n",
    "cv_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine 결론 **(모델 후보 선정)**\n",
    "- 모든 score들이 매우 높아, 잘 예측함을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "6. Random Forest\n",
    "   - max_depth 평가, criterion 평가 (gini, entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 50, 100],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "clf, cv_result = grid_search_cv(SVC, X_train_scaled, y_train, param_grid)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)\n",
    "cv_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "7. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(random_state=0)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost 결론 **(모델 후보 탈락)**\n",
    "- class 0번과 4번만 다 맞추고 나머지 class는 예측 실패"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "8. GradientBoosting\n",
    "   - max_depth 평가, learning rate 평가\n",
    "   - 해당 모형은 20분씩 걸리므로 차원 축소 방법을 적용해서 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_scaled, y_train)\n",
    "X_train_lda = lda.transform(X_train_scaled)\n",
    "X_test_lda = lda.transform(X_test_scaled)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(max_depth=2, learning_rate=0.1, random_state=0)\n",
    "clf.fit(X_train_lda, y_train)\n",
    "pred = clf.predict(X_test_lda)\n",
    "print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0번, 3번 class 정확도가 높은데, 나머지는 60% 안팍이다.\n",
    "- 약한 학습기의 조합으로 좋은 결과를 내는 배깅에 사용 해볼 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(X_train_scaled, y_train)\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(max_depth=2, learning_rate=0.1, random_state=0)\n",
    "clf.fit(X_train_pca, y_train)\n",
    "pred = clf.predict(X_test_pca)\n",
    "print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA n_components에 따라 score 많이 개선되는 것을 확인\n",
    "- for loop를 이용해 n_components 값을 정하고 GradientBoosting에서 StratifiedKFold + GridSearchCV 사용해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "best_clf = GradientBoostingClassifier()\n",
    "best_component = 0\n",
    "for n in range(10, 110, 10):\n",
    "    max_val = 0\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X_train_scaled, y_train)\n",
    "    X_train_pca = pca.transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    clf = GradientBoostingClassifier(max_depth=2, learning_rate=0.1, random_state=0)\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    pred = clf.predict(X_test_pca)\n",
    "    \n",
    "    if max_val < sum(pred == y_test):\n",
    "       max_val = sum(pred == y_test)\n",
    "       best_component = n\n",
    "       best_clf = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoosting 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "9. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "pred = clf.predict(X_test_scaled)\n",
    "print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA 결론 **(모델 후보 선정)**\n",
    "- 전체적으로 0번과 3번 class에 대해서 정확도 높게 예측\n",
    "- 단독 모델로 사용하기 어렵지만, 앙상블 모델에 개별 분류기로 적용해보려 함\n",
    "- 50% 언저리이므로 약한 학습기의 조합으로 좋은 결과를 내는 배깅에 사용 해볼 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "10. Voting\n",
    "   - estimator 평가 (위 모델들 조합), voting 평가 (soft, hard), weights 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# clf = VotingClassifier(\n",
    "#     estimators=[\n",
    "#         ('logistic_regression', LogisticRegression()),\n",
    "#         ('svc', SVC(kernel='linear')),\n",
    "#         ('gaussianNB', GaussianNB())\n",
    "#     ],\n",
    "#     voting='hard',\n",
    "#     weights=[1, 1, 1]\n",
    "# )\n",
    "# clf.fit(X_train_scaled, y_train)\n",
    "# pred = clf.predict(X_test_scaled)\n",
    "# print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "11. Bagging\n",
    "   - estimator 평가 (위 모델들 조합), n_estimators 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# clf = BaggingClassifier(estimator=GaussianNB(),\n",
    "#                         n_estimators=10,\n",
    "#                         random_state=0)\n",
    "# clf.fit(X_train_scaled, y_train)\n",
    "# pred = clf.predict(X_test_scaled)\n",
    "# print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "12. Stacking\n",
    "   - estimator, final estimator 평가 (위 모델들 조합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "# clf1 = SVC(kernel='linear', random_state=1)\n",
    "# clf2 = GaussianNB()\n",
    "# clf = StackingClassifier(\n",
    "#     estimators=[\n",
    "#         ('svm', clf1),\n",
    "#         ('gnb', clf2)\n",
    "#     ],\n",
    "#     final_estimator=LogisticRegression())\n",
    "# clf.fit(X_train_scaled, y_train)\n",
    "# pred = clf.predict(X_test_scaled)\n",
    "# print_result_classifier(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 혼자 Test 해보기 끝\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **결론**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <결론 가이드>\n",
    "- 처음 정의한 문제 해결\n",
    "- 문제와 관련된 새로운 사실\n",
    "- 기존 업무 효율화\n",
    "- 데이터셋에 대한 인사이트\n",
    "- 등등\n",
    "\n",
    "여러 관점에서 프로젝트의 결과를 정리하여 최종 결론을 논리적으로 서술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추천 시스템을 통해 구매 전환율이 예상보다 X% 향상될 것으로 예측합니다.\n",
    "\n",
    "추가 데이터 수집 및 다른 알고리즘의 적용 가능성에 대해서도 탐색합니다.\n",
    "\n",
    "더 정밀한 타겟팅과 개인화된 추천을 위해, 향후에는 고급 기법들을 도입할 계획입니다. 예를 들어, 딥러닝 기반의 추천 시스템이나, 시간에 따라 변화하는 고객의 선호를 반영할 수 있는 동적 모델링 방법들입니다.\n",
    "\n",
    "이 프로젝트를 통해 얻은 인사이트와 모델은 실제 업무 프로세스에 통합될 예정입니다. 예를 들어, 마케팅 캠페인의 타겟팅 전략 수립, 재고 관리 및 물류 계획의 최적화, 신제품 개발의 방향성 결정 등 다양한 분야에서 활용될 수 있습니다.\n",
    "또한, 프로젝트 결과는 내부 리포트와 워크숍을 통해 공유되어, 조직 전반의 데이터 기반 의사결정 문화를 강화하는 데 기여할 것입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_11_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
