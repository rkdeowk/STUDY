{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, LeakyReLU, BatchNormalization, Input, Reshape, concatenate\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(img, num_images=5):\n",
    "    total_images = len(img)\n",
    "    num_rows = int(np.ceil(total_images / num_images))  # 필요한 행의 수 계산\n",
    "    \n",
    "    # 서브플롯 생성 (num_rows 행과 num_images 열)\n",
    "    fig, axes = plt.subplots(num_rows, num_images, figsize=(num_images * 3, num_rows * 3))\n",
    "    \n",
    "    # 만약 num_rows가 1이라면, axes를 1차원 배열로 처리하도록 설정\n",
    "    if num_rows == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "    \n",
    "    # 이미지 그리기\n",
    "    for i in range(total_images):\n",
    "        row = i // num_images  # 현재 행\n",
    "        col = i % num_images  # 현재 열\n",
    "        axes[row, col].imshow(img[i].squeeze(), cmap='gray')  # 이미지 표시\n",
    "        axes[row, col].set_title(f\"Image {i+1}\")  # 이미지 제목 설정\n",
    "        axes[row, col].axis('off')  # 축 숨기기\n",
    "    \n",
    "    # 남은 빈 서브플롯 숨기기\n",
    "    for i in range(total_images, num_rows * num_images):\n",
    "        row = i // num_images\n",
    "        col = i % num_images\n",
    "        axes[row, col].axis('off')  # 빈 칸은 숨기기\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FDC 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('./data/fdc_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "sensor_data = df_final.iloc[:,5:]\n",
    "sensor_data_scaled = scaler.fit_transform(sensor_data)\n",
    "df_sensor_scaled = pd.DataFrame(sensor_data_scaled, columns=sensor_data.columns)\n",
    "df_fdc_scaled = pd.concat([df_final.iloc[:,:5].reset_index(drop=True), df_sensor_scaled.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라벨 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_fdc_scaled['PASS(1)/FAIL(0)'].values\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = np.load('./data/image_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1060, 128, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델\n",
    "\n",
    "1. FDC data → Image 예측\n",
    "\n",
    "2. Image → Scratch 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Validation / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdc_train, fdc_temp, img_train, img_temp, labels_train, labels_temp = train_test_split(\n",
    "    df_sensor_scaled, image_data, labels, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "fdc_val, fdc_test, img_val, img_test, labels_val, labels_test = train_test_split(\n",
    "    fdc_temp, img_temp, labels_temp, test_size=0.25, random_state=SEED  # 0.25 of the remaining 20% = 5% of total\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((848, 216),\n",
       " (159, 216),\n",
       " (53, 216),\n",
       " (848, 128, 128),\n",
       " (159, 128, 128),\n",
       " (53, 128, 128))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdc_train.shape, fdc_val.shape, fdc_test.shape, img_train.shape, img_val.shape, img_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 (기존 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "0.4339622641509434\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "autoencoder = tf.keras.models.load_model(f'./data/model/autoencoder_model_{epochs}.keras')\n",
    "classification_model = tf.keras.models.load_model(f'./data/model/cnn_model_{epochs}.keras')\n",
    "\n",
    "# 예측에 사용할 데이터 (예: fdc_test)\n",
    "fdc_test_data = fdc_test  # FDC 테스트 데이터를 준비합니다.\n",
    "\n",
    "predicted_images = autoencoder.predict(fdc_test_data)\n",
    "\n",
    "def predict_defect_classification(classification_model, data):\n",
    "    predicted_labels = classification_model.predict(data)\n",
    "    predicted_classes = (predicted_labels > 0.5).astype(int)  # 0.5 기준으로 이진 분류 (0 또는 1)\n",
    "    return predicted_classes\n",
    "\n",
    "# 예측된 Normal/Defective 클래스\n",
    "predicted_classes = predict_defect_classification(classification_model, predicted_images)\n",
    "\n",
    "# 예측 결과 출력\n",
    "# visualize_images(predicted_images)\n",
    "\n",
    "# 실제 라벨 (예: 테스트 데이터셋의 라벨)\n",
    "true_labels = labels_test  # 실제 라벨이 들어있는 labels_test 변수\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(true_labels, predicted_classes)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 (Concatenate 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018015D9BE20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 227ms/stepWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018015D9BE20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372ms/step\n",
      "0.6415094339622641\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "autoencoder = tf.keras.models.load_model(f'./data/model/autoencoder_model_{epochs}.keras')\n",
    "classification_model = tf.keras.models.load_model(f'./data/model/classification_model_{epochs}.keras')\n",
    "\n",
    "fdc_test_data = fdc_test  # FDC 테스트 데이터를 준비합니다.\n",
    "\n",
    "predicted_images = autoencoder.predict(fdc_test_data)\n",
    "\n",
    "def predict_defect_classification(classification_model, data):\n",
    "    predicted_labels = classification_model.predict(data)\n",
    "    predicted_classes = (predicted_labels > 0.5).astype(int)  # 0.5 기준으로 이진 분류 (0 또는 1)\n",
    "    return predicted_classes\n",
    "\n",
    "# 예측된 Normal/Defective 클래스\n",
    "predicted_classes = predict_defect_classification(classification_model, fdc_test_data)\n",
    "\n",
    "# 예측 결과 출력\n",
    "# visualize_images(predicted_images)\n",
    "\n",
    "# 실제 라벨 (예: 테스트 데이터셋의 라벨)\n",
    "true_labels = labels_test  # 실제 라벨이 들어있는 labels_test 변수\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(true_labels, predicted_classes)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. FDC data → Image 예측 모델 만들기 (autoencoder 모델, 기존)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoder 모델: 입력 이미지를 잠재 공간으로 압축\n",
    "# def build_encoder(input_dim):\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(layers.Dense(256, input_dim=input_dim))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    \n",
    "#     model.add(layers.Dense(512))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    \n",
    "#     model.add(layers.Dense(1024))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    \n",
    "#     model.add(layers.Dense(2048))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization(momentum=0.8))\n",
    "\n",
    "    \n",
    "#     # 잠재 공간 (저차원 공간) 출력\n",
    "#     model.add(layers.Dense(64))  # 잠재 공간의 크기를 64로 설정\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Decoder 모델: 잠재 공간에서 128x128 이미지를 복원\n",
    "# def build_decoder(latent_dim):\n",
    "#     model = tf.keras.Sequential()\n",
    "\n",
    "#     model.add(layers.Dense(2048))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    \n",
    "#     model.add(layers.Dense(1024, input_dim=latent_dim))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    \n",
    "#     model.add(layers.Dense(512))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "#     model.add(layers.Dense(256))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "#     # 128x128 이미지로 복원\n",
    "#     model.add(layers.Dense(128 * 128, activation='sigmoid'))  # 이미지 픽셀 값이 0-1 범위이므로 sigmoid 사용\n",
    "#     model.add(layers.Reshape((128, 128, 1)))\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Autoencoder 모델: Encoder와 Decoder를 연결\n",
    "# def build_autoencoder(input_dim, latent_dim):\n",
    "#     encoder = build_encoder(input_dim)\n",
    "#     decoder = build_decoder(latent_dim)\n",
    "    \n",
    "#     inputs = layers.Input(shape=(input_dim,))\n",
    "#     encoded = encoder(inputs)\n",
    "#     decoded = decoder(encoded)\n",
    "    \n",
    "#     autoencoder = tf.keras.Model(inputs, decoded)\n",
    "#     autoencoder.compile(optimizer='adam', loss='mse')  # Autoencoder에서는 MSE(평균 제곱 오차)를 사용\n",
    "#     return autoencoder\n",
    "\n",
    "# # 하이퍼파라미터\n",
    "# tf.random.set_seed(SEED)\n",
    "# input_dim = fdc_train.shape[1]  # FDC 데이터의 차원\n",
    "# latent_dim = 64  # 잠재 공간의 크기\n",
    "# epochs = 1000\n",
    "# batch_size = 32\n",
    "\n",
    "# # 모델 빌드\n",
    "# autoencoder = build_autoencoder(input_dim, latent_dim)\n",
    "\n",
    "# # 학습 함수\n",
    "# def train_autoencoder(epochs, batch_size):\n",
    "#     autoencoder.fit(fdc_train, img_train, epochs=epochs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Autoencoder 학습 시작\n",
    "# train_autoencoder(epochs, batch_size)\n",
    "\n",
    "# # 모델 저장\n",
    "# autoencoder.save(f'./data/model/autoencoder_model_{epochs}_original.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Image 분류 모델 만들기 (CNN 모델, 기존)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 외곽 부분을 마스킹하는 함수 정의 (원형 마스크 사용)\n",
    "# def create_edge_mask(image, radius):\n",
    "#     h, w = image.shape[:2]\n",
    "#     center = (int(w / 2), int(h / 2))\n",
    "#     max_radius = min(center) - 2  # 이미지 경계 안에 들어가도록 반지름 설정\n",
    "#     mask = np.zeros((h, w), dtype=np.uint8)  # 마스크를 8비트 정수로 생성\n",
    "    \n",
    "#     # 원형 마스크 생성 (두께는 외곽 영역의 범위를 조절)\n",
    "#     cv2.circle(mask, center, max_radius, 1, thickness=radius)  # 마스크 값 1로 설정\n",
    "#     return mask\n",
    "\n",
    "# # 이미지 데이터 (img_train을 불러온 상태라 가정)\n",
    "# image = img_train[0]  # 첫 번째 훈련 이미지 사용\n",
    "\n",
    "# # 다양한 radius 값으로 마스크를 생성하고 시각적으로 확인\n",
    "# radius_values = [1,5, 10, 15, 20, 50, 100]  # 다양한 반경 값을 설정\n",
    "# fig, axes = plt.subplots(1, len(radius_values), figsize=(15, 5))\n",
    "\n",
    "# for i, radius in enumerate(radius_values):\n",
    "#     # 마스크 생성\n",
    "#     mask = create_edge_mask(image, radius=radius)\n",
    "    \n",
    "#     # 마스크를 이미지에 적용하여 시각적으로 비교\n",
    "#     masked_image = cv2.bitwise_and(image.squeeze().astype(np.uint8), image.squeeze().astype(np.uint8), mask=mask)\n",
    "    \n",
    "#     # 이미지 출력\n",
    "#     axes[i].imshow(masked_image, cmap='gray')\n",
    "#     axes[i].set_title(f\"Radius: {radius}\")\n",
    "#     axes[i].axis('off')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # 외곽 부분을 마스킹하는 함수 정의 (원형 마스크 사용)\n",
    "# def create_edge_mask(image, radius=50):\n",
    "#     h, w = image.shape[:2]\n",
    "#     center = (int(w / 2), int(h / 2))\n",
    "#     max_radius = min(center) - 2  # 이미지 경계 안에 들어가도록 반지름 설정\n",
    "#     mask = np.zeros((h, w), dtype=np.uint8)  # 마스크를 uint8 형식으로 생성\n",
    "\n",
    "#     # 원형 마스크 생성 (두께는 외곽 영역의 범위를 조절)\n",
    "#     cv2.circle(mask, center, max_radius, 1, thickness=radius)  # 1로 채운 원형 마스크\n",
    "#     return mask\n",
    "\n",
    "# # 모든 이미지에 마스크 적용\n",
    "# def apply_mask_to_images(images, mask_radius=50):\n",
    "#     masked_images = []\n",
    "#     for image in images:\n",
    "#         mask = create_edge_mask(image.squeeze(), radius=mask_radius)\n",
    "        \n",
    "#         # 이미지가 0~1로 스케일링된 경우 0~255 범위로 변환\n",
    "#         image_uint8 = (image.squeeze() * 255).astype(np.uint8)\n",
    "        \n",
    "#         # 마스크 적용\n",
    "#         masked_image = cv2.bitwise_and(image_uint8, image_uint8, mask=mask)\n",
    "#         masked_images.append(masked_image)\n",
    "#     return np.array(masked_images)\n",
    "\n",
    "# # 훈련, 검증, 테스트 데이터 각각에 마스크 적용\n",
    "# masked_train = apply_mask_to_images(img_train)\n",
    "# masked_val = apply_mask_to_images(img_val)\n",
    "# masked_test = apply_mask_to_images(img_test)\n",
    "\n",
    "# # CNN 모델 생성\n",
    "# def build_edge_classification_model(input_shape):\n",
    "#     model = models.Sequential()\n",
    "    \n",
    "#     # Convolutional layer로 특징 추출\n",
    "#     model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "#     # Flatten하여 Dense Layer에 연결\n",
    "#     model.add(layers.Flatten())\n",
    "#     model.add(layers.Dense(128, activation='relu'))\n",
    "    \n",
    "#     # 이진 분류를 위한 출력층\n",
    "#     model.add(layers.Dense(1, activation='sigmoid'))  # 이진 분류\n",
    "    \n",
    "#     # 모델 컴파일\n",
    "#     model.compile(optimizer='adam',\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # 입력 이미지의 크기 (128x128 크기의 흑백 이미지)\n",
    "# input_shape = (128, 128, 1)\n",
    "# cnn = build_edge_classification_model(input_shape)\n",
    "\n",
    "# epochs = 1000\n",
    "\n",
    "# # 훈련 데이터를 이용해 모델 학습\n",
    "# cnn.fit(masked_train, labels_train, \n",
    "#           epochs=epochs, \n",
    "#           batch_size=32, \n",
    "#           validation_data=(masked_val, labels_val))\n",
    "\n",
    "# # 모델 저장\n",
    "# cnn.save(f'./data/model/cnn_model_{epochs}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Concatenate 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoder 모델: 입력 Tabular 데이터를 잠재 공간으로 압축\n",
    "# def build_encoder(input_dim):\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(layers.Dense(256, input_dim=input_dim))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    \n",
    "#     model.add(layers.Dense(512))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    \n",
    "#     model.add(layers.Dense(1024))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    \n",
    "#     model.add(layers.Dense(2048))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization(momentum=0.8))\n",
    "\n",
    "#     # 잠재 공간 출력\n",
    "#     model.add(layers.Dense(64))  # 잠재 공간의 크기를 64로 설정\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Decoder 모델: 잠재 공간에서 128x128 이미지를 복원\n",
    "# def build_decoder(latent_dim):\n",
    "#     model = tf.keras.Sequential()\n",
    "\n",
    "#     model.add(layers.Dense(2048, input_dim=latent_dim))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    \n",
    "#     model.add(layers.Dense(1024))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "#     model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    \n",
    "#     model.add(layers.Dense(512))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "#     model.add(layers.Dense(256))\n",
    "#     model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "#     # 128x128 이미지로 복원\n",
    "#     model.add(layers.Dense(128 * 128, activation='sigmoid'))\n",
    "#     model.add(layers.Reshape((128, 128, 1)))\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Autoencoder 모델: Encoder와 Decoder를 연결\n",
    "# def build_autoencoder(input_dim, latent_dim):\n",
    "#     encoder = build_encoder(input_dim)\n",
    "#     decoder = build_decoder(latent_dim)\n",
    "    \n",
    "#     inputs = layers.Input(shape=(input_dim,))\n",
    "#     encoded = encoder(inputs)\n",
    "#     decoded = decoder(encoded)\n",
    "    \n",
    "#     autoencoder = tf.keras.Model(inputs, decoded)\n",
    "#     autoencoder.compile(optimizer='adam', loss='mse')\n",
    "#     return autoencoder\n",
    "\n",
    "# def build_classification_model(input_dim, latent_dim):\n",
    "#     autoencoder = build_autoencoder(input_dim, latent_dim)\n",
    "   \n",
    "#     # Tabular 데이터를 위한 입력\n",
    "#     tabular_input = layers.Input(shape=(input_dim,))\n",
    "#     # AutoEncoder로 생성된 이미지\n",
    "#     decoded_image = autoencoder(tabular_input)\n",
    "    \n",
    "#     # 분류 목적의 새로운 피처 추출 레이어 추가\n",
    "#     feature_extraction = layers.Dense(512, activation='relu')(tabular_input)\n",
    "#     feature_extraction = layers.BatchNormalization()(feature_extraction)\n",
    "#     feature_extraction = layers.Dense(256, activation='relu')(feature_extraction)\n",
    "#     feature_extraction = layers.BatchNormalization()(feature_extraction)\n",
    "    \n",
    "#     # AutoEncoder 이미지와 새로운 피처 추출 레이어를 결합\n",
    "#     flat_image = layers.Flatten()(decoded_image)\n",
    "#     concat_features = layers.concatenate([feature_extraction, flat_image])\n",
    "    \n",
    "#     # Classification Layer\n",
    "#     x = layers.Dense(512, activation='relu')(concat_features)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dense(256, activation='relu')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "#     output = layers.Dense(1, activation='sigmoid')(x)  # 이진 분류를 가정\n",
    "    \n",
    "#     model = tf.keras.Model(inputs=tabular_input, outputs=output)\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "#     return model, autoencoder\n",
    "\n",
    "# # 하이퍼파라미터\n",
    "# tf.random.set_seed(SEED)\n",
    "# input_dim = fdc_train.shape[1]  # FDC 데이터의 차원\n",
    "# latent_dim = 64  # 잠재 공간의 크기\n",
    "# epochs = 1000\n",
    "# batch_size = 32\n",
    "\n",
    "# # 모델 빌드 및 학습\n",
    "# classification_model, autoencoder = build_classification_model(input_dim, latent_dim)\n",
    "\n",
    "# # AutoEncoder 학습\n",
    "# autoencoder.fit(fdc_train, img_train, epochs=epochs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # 분류 모델 학습\n",
    "# classification_model.fit(fdc_train, labels_train, epochs=epochs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # 모델 저장\n",
    "# autoencoder.save(f'./data/model/autoencoder_model_{epochs}.keras')\n",
    "# classification_model.save(f'./data/model/classification_model_{epochs}.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
